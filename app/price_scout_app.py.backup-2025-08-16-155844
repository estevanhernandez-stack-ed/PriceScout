# price_scout_app.py - The Streamlit User Interface & Scraping Engine (v21.0 - Final Refactor)

import streamlit as st
import pandas as pd
import os
import datetime
import time
import asyncio
import traceback
import sys
import threading
import urllib.parse
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import re
import json
from functools import reduce
import io
from contextlib import redirect_stdout

# --- Dynamically Define File Paths ---
# Get the absolute path to the directory where this script is located (e.g., /root/PriceScout/app)
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# Get the main project directory, which is one level up from the script's directory
PROJECT_DIR = os.path.dirname(SCRIPT_DIR)

# --- Check for Developer Mode ---
# This checks the URL for a query parameter like "?dev=true"
query_params = st.query_params
DEV_MODE_ENABLED = query_params.get("dev") == "true"

# --- Streamlit Page Configuration ---
st.set_page_config(
    page_title="PriceScout",
    page_icon=os.path.join(SCRIPT_DIR, 'PriceScoutLogo.png'), # <-- ADD THIS LINE
    layout="wide"
)

# --- Custom CSS for Marcus Theatres Red Buttons (Final Definitive Version) ---
st.markdown("""
<style>
    /* --- Base Style --- */
    button {
        border-radius: 5px !important;
        font-weight: bold !important;
        transition: background-color 0.2s, color 0.2s, border-color 0.2s;
    }

    /* --- Targets the UNSELECTED button with its specific class --- */
    button.st-emotion-cache-1anq8dj.e7nj0r42 {
        border: 2px solid #A21E25 !important;
        color: #A21E25 !important;
        background-color: transparent !important;
    }

    /* --- Targets the SELECTED button with its specific class --- */
    button.st-emotion-cache-1krtkoa.e7nj0r41 {
        background-color: #8B0F05 !important; /* Our dark red */
        color: white !important;
        border: 1px solid #8B0F05 !important;
    }

    /* --- HOVER Style for both buttons --- */
    button.st-emotion-cache-1anq8dj.e7nj0r42:hover,
    button.st-emotion-cache-1krtkoa.e7nj0r41:hover {
        background-color: #A21E25 !important; /* Our muted red */
        color: white !important;
        border-color: #A21E25 !important;
    }
    
    /* --- Focus Style (for accessibility) --- */
    button:focus {
        box-shadow: 0 0 0 0.2rem rgba(139, 15, 5, 0.5) !important;
    }
</style>
""", unsafe_allow_html=True)


def check_password():
    """Returns `True` if the user has the correct password."""
    
    # Check if a password is set in the secrets file. If not, skip the check.
    if "password" not in st.secrets:
        return True

    def password_entered():
        """Checks whether a password entered by the user is correct."""
        if st.session_state["password"] == st.secrets["password"]:
            st.session_state["password_correct"] = True
            del st.session_state["password"]  # Don't store the password.
        else:
            st.session_state["password_correct"] = False

    if st.session_state.get("password_correct", False):
        return True

    # Show input for password.
    st.text_input(
        "Password", type="password", on_change=password_entered, key="password"
    )
    if "password_correct" in st.session_state:
        st.error("😕 Password incorrect")
    return False

# --- Main Application Logic ---
if check_password():
    st.title('📊 PriceScout: Competitive Pricing Tool')

    # --- Constants ---
    CACHE_FILE = os.path.join(SCRIPT_DIR, 'theater_cache.json')
    MARKETS_FILE = os.path.join(PROJECT_DIR, 'data', 'Marcus', 'markets.json')
    CACHE_EXPIRATION_DAYS = 3
    REPORTS_DIR = os.path.join(PROJECT_DIR, 'data', 'Marcus', 'reports') # Base directory for reports

    # --- Data Scraping Engine (Scraper Class) ---
    class Scraper:
        def _sanitize_for_comparison(self, text: str) -> str:
            text = text.lower()
            text = re.sub(r'[^a-z0-9\s]', '', text)
            text = re.sub(r'\s\d+\s', ' ', text)
            text = re.sub(r'\s+', ' ', text).strip()
            return text

        def _sanitize_filename(self, name):
            return re.sub(r'[\\/*?:"<>|]',"", name).replace(" ", "_")

        def _clean_ticket_type(self, description: str) -> str:
            desc_lower = description.lower()
            known_types = {'adult': 'Adult', 'child': 'Child', 'senior': 'Senior', 'military': 'Military', 'student': 'Student'}
            base_type = next((v for k, v in known_types.items() if k in desc_lower), None)
            amenities = []
            if 'recliner' in desc_lower: amenities.append('Recliner')
            if 'd-box' in desc_lower: amenities.append('D-BOX')
            if 'xd' in desc_lower: amenities.append('XD')
            if 'luxury' in desc_lower: amenities.append('Luxury')
            if 'imax' in desc_lower: amenities.append('IMAX')
            if 'dolby' in desc_lower: amenities.append('Dolby Cinema')
            if 'promotion' in desc_lower or 'tuesday' in desc_lower: amenities.append('Promotion')
            if base_type:
                return f"{base_type} ({', '.join(sorted(list(set(amenities))))})" if amenities else base_type
            return description.split('(')[0].strip()

        def _classify_daypart(self, showtime_str: str) -> str:
            try:
                s = showtime_str.strip().lower().replace('.', '')
                # normalize common variants: "7:30pm", "7:30 p", "7:30 p.m."
                s = s.replace('p.m.', 'pm').replace('a.m.', 'am').replace('p ', 'pm').replace('a ', 'am')
                if not any(x in s for x in ('am','pm')):
                    # if hour is a high number (e.g. 8-11) default to pm, else am
                    hour = int(re.match(r'(\d{1,2})', s).group(1))
                    s += 'am' if hour < 8 else 'pm'
                t = datetime.datetime.strptime(s, "%I:%M%p").time()
                if t < datetime.time(16,0): return "Matinee"
                if t <= datetime.time(21,0): return "Prime"
                return "Late Night"
            except Exception:
                return "Unknown"

        async def _get_theaters_from_zip_page(self, page, zip_code):
            url = f"https://www.fandango.com/{zip_code}_movietimes"
            print(f"  - Checking ZIP: {zip_code}")
            try:
                await page.goto(url, timeout=30000)
                await page.mouse.wheel(0, 2000)
                await page.wait_for_timeout(1500)
                js_condition = "() => window.Fandango && window.Fandango.pageDetails && window.Fandango.pageDetails.localTheaters && window.Fandango.pageDetails.localTheaters.length > 0"
                await page.wait_for_function(js_condition, timeout=20000)
                theaters_data = await page.evaluate('() => window.Fandango.pageDetails.localTheaters')
                return {t.get('name'): {"name": t.get('name'), "url": "https://www.fandango.com" + t.get('theaterPageUrl')} for t in theaters_data if t.get('name') and t.get('theaterPageUrl')}
            except Exception as e:
                print(f"    [WARNING] Could not process ZIP {zip_code}. Error: {e}")
                return {}
                
        async def live_search_by_zip(self, zip_code):
            """Launches a browser to perform a live ZIP search."""
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                results = await self._get_theaters_from_zip_page(page, zip_code)
                await browser.close()
                return results

        async def live_search_by_name(self, search_term):
            """Launches a browser to perform a live name search on Fandango."""
            print(f"  - Live searching for: {search_term}")
            results = {}
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                try:
                    await page.goto("https://www.fandango.com", timeout=60000)
                    await page.locator('[data-qa="search-input"]').fill(search_term)
                    await page.locator('[data-qa="search-input"]').press('Enter')
                    await page.wait_for_selector('[data-qa="search-results-item"]', timeout=15000)
                    
                    soup = BeautifulSoup(await page.content(), 'html.parser')
                    search_results_items = soup.select('[data-qa="search-results-item"]')
                    for item in search_results_items:
                        link_elem = item.select_one('a[data-qa="search-results-item-link"]')
                        if link_elem and '/theater-page' in link_elem.get('href', ''):
                            name = link_elem.get_text(strip=True)
                            url = "https://www.fandango.com" + link_elem['href']
                            results[name] = {"name": name, "url": url}
                except Exception as e:
                    print(f"    [WARNING] Could not complete live name search. Error: {e}")
                
                await browser.close()
                return results

        async def build_theater_cache(self, markets_json_path):
            with open(markets_json_path, 'r') as f:
                markets_data = json.load(f)
            theaters_by_market = {}
            for parent_company, regions in markets_data.items():
                for region_name, markets in regions.items():
                    for market_name, market_info in markets.items():
                        if market_name not in theaters_by_market:
                            theaters_by_market[market_name] = {'theaters': [], 'zip_pool': set()}
                        for theater in market_info['theaters']:
                            theaters_by_market[market_name]['theaters'].append(theater)
                            if theater.get('zip'):
                                theaters_by_market[market_name]['zip_pool'].add(theater.get('zip'))
            full_cache = {"metadata": {"last_updated": datetime.datetime.now().isoformat()}, "markets": {}}
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                for market_name, market_data in theaters_by_market.items():
                    print(f"\n--- Caching Market: {market_name} ---")
                    market_zip_cache = {}
                    for zip_code in market_data['zip_pool']:
                        zip_results = await self._get_theaters_from_zip_page(page, zip_code)
                        market_zip_cache.update(zip_results)
                    found_theaters_for_market = []
                    for theater_from_json in market_data['theaters']:
                        name = theater_from_json['name']
                        found = False
                        sanitized_target_name = self._sanitize_for_comparison(name)
                        for live_name, live_data in market_zip_cache.items():
                            sanitized_live_name = self._sanitize_for_comparison(live_name)
                            if sanitized_target_name in sanitized_live_name or sanitized_live_name in sanitized_target_name:
                                found_theaters_for_market.append({'name': live_name, 'url': live_data['url']})
                                found = True
                                break
                        if not found:
                            print(f"  [WARNING] Could not find '{name}' in cache for market '{market_name}'.")
                    full_cache["markets"][market_name] = {"theaters": found_theaters_for_market}
                await browser.close()
            with open(CACHE_FILE, 'w') as f:
                json.dump(full_cache, f, indent=2)
            return full_cache

        async def _get_movies_from_theater_page(self, page, theater, date):
            full_url = f"{theater['url']}?date={date}"
            await page.goto(full_url, timeout=60000)
            try:
                await page.locator('div.theater-presenting-formats, li.fd-panel').first.wait_for(timeout=30000)
            except Exception as e:
                print(f"[DEBUG] WARNING for {theater['name']}: Could not find a reliable load signal or timed out: {e}")
            html_content = await page.content()
            soup = BeautifulSoup(html_content, 'html.parser')
            showings = []
            movie_blocks = soup.select('li.fd-panel')
            for movie_block in movie_blocks:
                film_title_elem = movie_block.select_one('h2.thtr-mv-list__detail-title a')
                film_title = film_title_elem.get_text(strip=True) if film_title_elem else "Unknown Title"
                variant_title_elem = movie_block.select_one('.movie-variant-title')
                variant_title = variant_title_elem.get_text(strip=True) if variant_title_elem else None
                showtime_links = movie_block.select('ol.showtimes-btn-list a.showtime-btn')
                for link in showtime_links:
                    time_label_elem = link.select_one('.showtime-btn-label')
                    amenity_elem = link.select_one('.showtime-btn-amenity')
                    time_str = time_label_elem.get_text(strip=True) if time_label_elem else link.get_text(strip=True)
                    amenity_str = amenity_elem.get_text(strip=True) if amenity_elem else variant_title
                    movie_format = amenity_str if amenity_str else "2D"
                    ticket_url = "https://tickets.fandango.com/transaction/ticketing/mobile/jump.aspx" + link.get('href', '').split('jump.aspx')[-1]
                    if film_title != "Unknown Title" and time_str and ticket_url and re.match(r'\d{1,2}:\d{2}[ap]m?', time_str, re.IGNORECASE):
                        showings.append({"film_title": film_title, "format": movie_format, "showtime": time_str, "daypart": self._classify_daypart(time_str), "ticket_url": ticket_url})
            return showings

        async def _get_prices_and_capacity(self, page, showing_details):
            showtime_url = showing_details['ticket_url']
            results = {"prices": {}, "capacity": "N/A"}
            try:
                await page.goto(showtime_url, timeout=60000)
                await page.wait_for_timeout(2000)
                html_content = await page.content()
                soup = BeautifulSoup(html_content, 'html.parser')
                scripts = soup.find_all('script')
                for script in scripts:
                    if script.string and 'window.Commerce.models' in script.string:
                        script_content = script.string
                        start_text = 'window.Commerce.models = '
                        start_index = script_content.find(start_text)
                        if start_index != -1:
                            json_start = script_content.find('{', start_index)
                            open_braces, json_end = 0, -1
                            for i in range(json_start, len(script_content)):
                                if script_content[i] == '{': open_braces += 1
                                elif script_content[i] == '}': open_braces -= 1
                                if open_braces == 0:
                                    json_end = i + 1; break
                            if json_end != -1:
                                data = json.loads(script_content[json_start:json_end])
                                ticket_types = data.get('tickets', {}).get('seatingAreas', [{}])[0].get('ticketTypes', [])
                                for tt in ticket_types:
                                    description, price = tt.get('description'), tt.get('price')
                                    if description and price is not None:
                                        results["prices"][self._clean_ticket_type(description)] = f"${price:.2f}"
                                seating_info = data.get('seating', {})
                                total_seats = seating_info.get('totalSeats')
                                available_seats = seating_info.get('availableSeats')
                                if available_seats is not None and total_seats is not None:
                                    results["capacity"] = f"{available_seats} / {total_seats}"
                                if results["prices"]: return results
            except Exception as e:
                results["prices"]['Error'] = f'Scraping failed: {e}'
            return results

        async def get_all_showings_for_theaters(self, theaters, date, mode):
            showings_by_theater = {}
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                for i, theater in enumerate(theaters):
                    context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
                    page = await context.new_page()
                    showings = await self._get_movies_from_theater_page(page, theater, date)
                    if mode == "CompSnipe Mode":
                        for showing in showings:
                            showing['theater_name'] = theater['name']
                    await context.close()
                    showings_by_theater[theater['name']] = showings
            return showings_by_theater

        async def scrape_details(self, theaters, selected_showtimes, date):
            all_price_data = []
            showings_to_scrape = []
            for theater in theaters:
                if theater['name'] in selected_showtimes:
                    for film, times in selected_showtimes[theater['name']].items():
                        for time, showing_info in times.items():
                            showings_to_scrape.append({**showing_info, "theater_name": theater['name']})
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                for showing in showings_to_scrape:
                    context_price = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
                    page_price = await context_price.new_page()
                    scrape_results = await self._get_prices_and_capacity(page_price, showing)
                    await context_price.close()
                    for ticket_type, price in scrape_results['prices'].items():
                        if "Error" not in ticket_type:
                            all_price_data.append({
                                "Theater Name": showing['theater_name'], "Film Title": showing['film_title'], 
                                "Format": showing['format'], "Showtime": showing['showtime'], 
                                "Daypart": showing['daypart'], "Ticket Type": ticket_type, "Price": price,
                                "Capacity": scrape_results['capacity']
                            })
            return all_price_data

        async def run_diagnostic_scrape(self, markets_to_test, date):
            """Runs a fast scrape on the first available showtime for all theaters in the selected markets."""
            diagnostic_results = []
            theaters_to_test = []
            with open(CACHE_FILE, 'r') as f:
                cache = json.load(f)
            for market_name in markets_to_test:
                theaters = cache.get("markets", {}).get(market_name, {}).get("theaters", [])
                for theater in theaters:
                    theater['market'] = market_name
                    theaters_to_test.append(theater)
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                for i, theater in enumerate(theaters_to_test):
                    # This function is now decoupled from Streamlit UI elements
                    print(f"Testing {i+1}/{len(theaters_to_test)}: {theater['name']}")
                    
                    context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
                    page = await context.new_page()
                    result_row = {"Market": theater['market'], "Theater Name": theater['name'], "Status": "Failed", "Details": "No showtimes found", "Sample Price": "N/A"}
                    try:
                        showings = await self._get_movies_from_theater_page(page, theater, date)
                        if showings:
                            first_showing = showings[0]
                            price_results = await self._get_prices_and_capacity(page, first_showing)
                            if price_results['prices'] and "Error" not in price_results['prices']:
                                first_price_type = list(price_results['prices'].keys())[0]
                                first_price_value = price_results['prices'][first_price_type]
                                result_row.update({
                                    "Status": "Success",
                                    "Details": f"Scraped '{first_showing['film_title']}' at {first_showing['showtime']}",
                                    "Sample Price": f"{first_price_type}: {first_price_value}"
                                })
                            else:
                                result_row["Details"] = "Failed to extract price from ticket page."
                        diagnostic_results.append(result_row)
                    except Exception as e:
                        result_row["Details"] = f"An unexpected error occurred: {str(e)}"
                        diagnostic_results.append(result_row)
                    finally:
                        await context.close()
            
            return diagnostic_results

    scout = Scraper()

    # --- UI Helper Functions ---
    def run_async_in_thread(target_func, *args, **kwargs):
        log_stream = io.StringIO()
        status, value = 'error', None
        def thread_target():
            nonlocal status, value
            if sys.platform == "win32":
                asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
            try:
                with redirect_stdout(log_stream):
                    result = asyncio.run(target_func(*args, **kwargs))
                status, value = 'success', result
            except Exception:
                error_str = traceback.format_exc()
                print(f"\n--- TRACEBACK ---\n{error_str}", file=log_stream)
                status, value = 'error', error_str
        thread = threading.Thread(target=thread_target)
        thread.start()
        thread.join()
        log_output = log_stream.getvalue()
        return status, value, log_output

    def check_cache_status():
        if not os.path.exists(CACHE_FILE):
            return "missing", None
        try:
            with open(CACHE_FILE, 'r') as f:
                cache_data = json.load(f)
            last_updated_str = cache_data.get("metadata", {}).get("last_updated")
            if not last_updated_str:
                return "invalid", None
            last_updated = datetime.datetime.fromisoformat(last_updated_str)
            if (datetime.datetime.now() - last_updated).days >= CACHE_EXPIRATION_DAYS:
                return "stale", last_updated.strftime('%Y-%m-%d %H:%M')
            return "fresh", last_updated.strftime('%Y-%m-%d %H:%M')
        except (json.JSONDecodeError, KeyError):
            return "invalid", None

    def get_report_path(mode, region=None, market=None):
        if mode == "Market Mode":
            # Creates a path like .../Reports/MarketMode/South_MT/Fort_Worth_76107/
            path = os.path.join(REPORTS_DIR, "MarketMode", scout._sanitize_filename(region or ""), scout._sanitize_filename(market or ""))
        elif mode == "CompSnipe Mode":
            # Creates a path like .../Reports/SnipeMode/
            path = os.path.join(REPORTS_DIR, "SnipeMode")
        else:
            path = os.path.join(REPORTS_DIR, "misc")
    
        os.makedirs(path, exist_ok=True)
        return path

    def reset_session():
        """Resets the Streamlit session state to start a new report."""
        keys_to_reset = [
            'stage', 'selected_region', 'selected_market', 'theaters', 
            'selected_theaters', 'all_showings', 'selected_films', 
            'selected_showtimes', 'confirm_scrape', 'compsnipe_theaters',
            'compsnipe_name_search_term', 'live_search_results', 'last_mode',
            'show_failsafe', 'run_diagnostic'
        ]
        for key in keys_to_reset:
            if key in st.session_state:
                del st.session_state[key]
        if 'final_df' in st.session_state:
            del st.session_state.final_df
        st.rerun()

    def get_missing_daypart_report(df):
        """Generates a list of warnings for theaters missing dayparts that were selected elsewhere."""
        if df.empty:
            return []
        all_selected_dayparts = set(df['Daypart'].unique())
        theaters_in_report = set(df['Theater Name'].unique())
        missing_entries = []
        for theater in sorted(list(theaters_in_report)):
            for daypart in sorted(list(all_selected_dayparts)):
                has_daypart = not df[(df['Theater Name'] == theater) & (df['Daypart'] == daypart)].empty
                if not has_daypart:
                    missing_entries.append(f"**{theater}**: Missing selection for **'{daypart}'** daypart.")
        return missing_entries

    def style_price_change(val):
        """Applies color styling to the Price_Change column based on its value."""
        if isinstance(val, str):
            if val.startswith('-$'):
                return 'color: green; font-weight: bold;'
            elif val.startswith('$') and val != '$0.00':
                return 'color: red; font-weight: bold;'
        return ''

    # --- Session State Initialization ---
    if 'stage' not in st.session_state: st.session_state.stage = 'initial'
    if 'search_mode' not in st.session_state: st.session_state.search_mode = "Market Mode"
    if 'last_run_log' not in st.session_state: st.session_state.last_run_log = ""
    if 'dev_mode' not in st.session_state: st.session_state.dev_mode = False
    if 'capture_html' not in st.session_state: st.session_state.capture_html = False
    if 'confirm_scrape' not in st.session_state: st.session_state.confirm_scrape = False
    if 'compsnipe_theaters' not in st.session_state: st.session_state.compsnipe_theaters = []
    if 'live_search_results' not in st.session_state: st.session_state.live_search_results = {}

    # --- Load initial data ---
    try:
        with open(MARKETS_FILE, 'r') as f:
            markets_data = json.load(f)
        with open(CACHE_FILE, 'r') as f:
            cache_data = json.load(f)
    except FileNotFoundError:
        st.error(f"Error: `{MARKETS_FILE}` or `{CACHE_FILE}` not found. Please ensure they are in the same directory.")
        st.info("You may need to build the theater cache first if it's missing.")
        st.stop()


    # --- Main UI ---
    st.sidebar.title("Controls")
    st.sidebar.image(os.path.join(SCRIPT_DIR, 'PriceScoutLogo.png'))
    if st.sidebar.button("🚀 Start New Report", use_container_width=True):
        reset_session()

    # --- ADD THIS NEW BLOCK ---
    st.sidebar.divider()

    # Conditionally show Developer Tools based on the URL parameter
    if DEV_MODE_ENABLED:
        st.sidebar.header("Developer Tools")
        st.session_state.dev_mode = True  # Set flag for other parts of the app

        st.session_state.capture_html = st.sidebar.toggle("Capture HTML Snapshots", help="Save HTML files for analysis.")
        if st.sidebar.button("Run Full System Diagnostic", use_container_width=True):
            st.session_state.run_diagnostic = True
            st.rerun()

    else:
        st.session_state.dev_mode = False # Ensure dev_mode is off for regular users

    # --- Diagnostic View ---
    if st.session_state.get('run_diagnostic'):
        st.header("🛠️ Full System Diagnostic")
        st.warning("**Warning:** This will scrape every theater in the selected markets and may take a very long time to complete.")
        
        all_markets = list(markets_data["Marcus Theatres"]["South MT"].keys()) + list(markets_data["Marcus Theatres"]["North MT"].keys())
        markets_to_test = st.multiselect("Select markets to test:", options=all_markets, default=all_markets)

        if st.button("Start Full Diagnostic", type="primary"):
            with st.spinner("Running diagnostic scan... This will take a long time."):
                diag_date_str = (datetime.date.today() + datetime.timedelta(days=1)).strftime('%Y-%m-%d')
                status, result, log = run_async_in_thread(scout.run_diagnostic_scrape, markets_to_test, diag_date_str)
                st.session_state.last_run_log += log

                if status == 'success':
                    st.success("Diagnostic Complete!")
                    df_diag = pd.DataFrame(result)
                    st.dataframe(df_diag)
                else:
                    st.error("The diagnostic tool encountered a critical error.")
        st.stop()


    st.subheader("Theater Data Cache")
    cache_status, last_updated = check_cache_status()
    if cache_status == "fresh":
        st.success(f"Theater cache is up to date. Last refreshed: {last_updated}")
    elif cache_status == "stale":
        st.warning(f"Theater cache is stale (older than {CACHE_EXPIRATION_DAYS} days). Last refreshed: {last_updated}")
    else:
        st.error("Theater cache file is missing or invalid. Please build it.")

    if st.button("Build / Refresh Theater Cache"):
        st.session_state.confirm_scrape = False
        with st.spinner("Building theater cache... This may take several minutes."):
            status, result, log = run_async_in_thread(scout.build_theater_cache, MARKETS_FILE)
            st.session_state.last_run_log = log
            if status == 'success':
                st.success("Theater cache built successfully!")
                st.rerun()
            else:
                st.error("Failed to build theater cache.")
    st.divider()

    st.subheader("Step 1: Define Search Area")
    st.session_state.search_mode = st.radio("Select Mode", ["Market Mode", "CompSnipe Mode"], horizontal=True, key="mode_selection")
    if st.session_state.search_mode != st.session_state.get('last_mode'):
        st.session_state.confirm_scrape = False 
        st.session_state.last_mode = st.session_state.search_mode


    # ==============================================================================
    # --- MARKET MODE ---
    # ==============================================================================
    if st.session_state.search_mode == "Market Mode":
        if 'selected_region' not in st.session_state: st.session_state.selected_region = None
        if 'selected_market' not in st.session_state: st.session_state.selected_market = None
        if 'theaters' not in st.session_state: st.session_state.theaters = []
        if 'selected_theaters' not in st.session_state: st.session_state.selected_theaters = []
        if 'all_showings' not in st.session_state: st.session_state.all_showings = {}
        if 'selected_films' not in st.session_state: st.session_state.selected_films = []
        if 'selected_showtimes' not in st.session_state: st.session_state.selected_showtimes = {}
        
        parent_company = list(markets_data.keys())[0]
        regions = list(markets_data[parent_company].keys())
        cols = st.columns(len(regions))
        for i, region in enumerate(regions):
            is_selected = st.session_state.selected_region == region
            if cols[i].button(region, key=f"region_{region}", type="primary" if is_selected else "secondary", use_container_width=True):
                st.session_state.selected_region = region
                st.session_state.selected_market = None
                st.session_state.stage = 'region_selected'
                st.rerun()

        if st.session_state.selected_region:
            st.markdown(f"### Region: {st.session_state.selected_region}")
            markets = list(markets_data[parent_company][st.session_state.selected_region].keys())
            st.write("Select a Market:")
            market_cols = st.columns(4)
            for i, market in enumerate(markets):
                is_selected = st.session_state.selected_market == market
                if market_cols[i % 4].button(market, key=f"market_{market}", type="primary" if is_selected else "secondary", use_container_width=True):
                    st.session_state.selected_market = market
                    st.session_state.theaters = cache_data.get("markets", {}).get(market, {}).get("theaters", [])
                    st.session_state.selected_theaters = [t['name'] for t in st.session_state.theaters]
                    st.session_state.stage = 'theaters_listed'
                    st.rerun()

        if st.session_state.stage in ['theaters_listed', 'data_fetched', 'report_generated']:
            st.subheader("Step 2: Select Theaters")
            cols = st.columns(4)
            for i, theater in enumerate(st.session_state.theaters):
                is_selected = theater['name'] in st.session_state.selected_theaters
                if cols[i % 4].button(theater['name'], key=f"theater_{i}", type="primary" if is_selected else "secondary", use_container_width=True):
                    if is_selected: st.session_state.selected_theaters.remove(theater['name'])
                    else: st.session_state.selected_theaters.append(theater['name'])
                    st.rerun()
            
            scrape_date = st.date_input("Select Date for Showtimes", datetime.date.today() + datetime.timedelta(days=1))
            scrape_date_str = scrape_date.strftime('%Y-%m-%d')

            if st.button("Find Films for Selected Theaters"):
                theaters_to_scrape = [t for t in st.session_state.theaters if t['name'] in st.session_state.selected_theaters]
                with st.spinner("Finding all available films and showtimes..."):
                    status, result, log = run_async_in_thread(scout.get_all_showings_for_theaters, theaters_to_scrape, scrape_date_str, "Market Mode")
                    st.session_state.last_run_log = log
                    if status == 'success':
                        st.session_state.all_showings = result
                        st.session_state.selected_films = []
                        st.session_state.selected_showtimes = {}
                        st.session_state.stage = 'data_fetched'
                    else: st.error("Failed to fetch showings for theaters.")
                st.rerun()

        if st.session_state.stage in ['data_fetched', 'report_generated']:
            st.subheader("Step 3: Select Films & Showtimes")
            
            theaters_awaiting = []
            if not st.session_state.get('selected_films', []):
                theaters_awaiting = sorted(st.session_state.get('selected_theaters', []))
            else:
                for theater_name in st.session_state.selected_theaters:
                    has_film = any(film in [s['film_title'] for s in st.session_state.all_showings.get(theater_name, [])] for film in st.session_state.selected_films)
                    if not has_film:
                        theaters_awaiting.append(theater_name)
            if theaters_awaiting:
                st.info("Theaters Awaiting a Film Selection:\n\n* " + "\n* ".join(sorted(theaters_awaiting)))

            all_films = sorted(list(reduce(lambda a, b: a.union(b), [set(s['film_title'] for s in showings) for showings in st.session_state.all_showings.values() if showings], set())))
            st.write("Select Films:")
            cols = st.columns(4)
            for i, film in enumerate(all_films):
                is_selected = film in st.session_state.selected_films
                if cols[i % 4].button(film, key=f"film_{film}", type="primary" if is_selected else "secondary", use_container_width=True):
                    if is_selected: st.session_state.selected_films.remove(film)
                    else: st.session_state.selected_films.append(film)
                    st.rerun()
            st.divider()

            def handle_showtime_click(theater_name, film_title, time_str, showing_info):
                if theater_name not in st.session_state.selected_showtimes: st.session_state.selected_showtimes[theater_name] = {}
                if film_title not in st.session_state.selected_showtimes[theater_name]: st.session_state.selected_showtimes[theater_name][film_title] = {}
                if time_str in st.session_state.selected_showtimes[theater_name][film_title]: del st.session_state.selected_showtimes[theater_name][film_title][time_str]
                else: st.session_state.selected_showtimes[theater_name][film_title][time_str] = showing_info

            for theater_name in st.session_state.selected_theaters:
                has_selections = any(st.session_state.selected_showtimes.get(theater_name, {}).values())
                expander_label = f"✅ {theater_name}" if has_selections else f"⚪️ {theater_name}"
                with st.expander(expander_label, expanded=True):
                    showings = st.session_state.all_showings.get(theater_name, [])
                    films_to_display = {f for f in st.session_state.selected_films if f in [s['film_title'] for s in showings]}
                    if not films_to_display: st.write("No selected films are showing at this theater.")
                    for film in sorted(list(films_to_display)):
                        st.markdown(f"**{film}**")
                        film_showings = sorted([s for s in showings if s['film_title'] == film], key=lambda x: datetime.datetime.strptime(x['showtime'].replace('p', 'PM').replace('a', 'AM'), "%I:%M%p").time())
                        cols = st.columns(8)
                        for i, showing in enumerate(film_showings):
                            time = showing['showtime']
                            is_selected = time in st.session_state.selected_showtimes.get(theater_name, {}).get(film, {})
                            if cols[i % 8].button(time, key=f"time_{theater_name}_{film}_{i}", type="primary" if is_selected else "secondary", use_container_width=True):
                                handle_showtime_click(theater_name, film, time, showing)
                                st.session_state.confirm_scrape = False
                                st.rerun()

        if any(any(film.values()) for film in st.session_state.get('selected_showtimes', {}).values()):
            st.subheader("Step 4: Generate Report")
            report_path = get_report_path(st.session_state.search_mode, st.session_state.get('selected_region'), st.session_state.get('selected_market'))
            previous_reports = []
            if report_path and os.path.exists(report_path):
                previous_reports = sorted([os.path.join(report_path, f) for f in os.listdir(report_path) if f.endswith('.csv')], reverse=True)
            selected_report_to_compare = st.selectbox("Compare to Previous Report (Optional):", ["None"] + previous_reports, help="Select a previous report to compare prices against.")
            if st.button('📄 Generate Live Pricing Report', use_container_width=True):
                st.session_state.confirm_scrape = True
                st.rerun()

    # ==============================================================================
    # --- COMPSNIPE MODE ---
    # ==============================================================================
    elif st.session_state.search_mode == "CompSnipe Mode":
        st.info("Use this mode for targeted analysis of individual theaters. Search for any theater by name, or use the ZIP code fallback to find new competitors.")
        
        name_col, btn_col = st.columns([4, 1])
        with name_col:
            name_search_term = st.text_input("Enter theater name to search on Fandango", key="compsnipe_name_search_term",
                                            on_change=lambda: st.session_state.update(show_failsafe=False, live_search_results={}))
        with btn_col:
            st.write("") 
            if st.button("Search by Name", key="search_by_name_btn"):
                st.session_state.live_search_results = {}
                if name_search_term:
                    with st.spinner(f"Live searching Fandango for '{name_search_term}'..."):
                        status, result, log = run_async_in_thread(scout.live_search_by_name, name_search_term)
                        st.session_state.last_run_log = log
                        if status == 'success':
                            st.session_state.live_search_results = result
                            if not result: st.session_state.show_failsafe = True
                        else: st.error("Failed to perform live name search.")
                else: st.warning("Please enter a name to search.")

        with st.expander("Can't find a theater? Search by ZIP code"):
            zip_col, zip_btn_col = st.columns([4, 1])
            with zip_col:
                zip_search_term = st.text_input("Enter 5-digit ZIP code", max_chars=5, key="zip_search_input",
                                                on_change=lambda: st.session_state.update(show_failsafe=False, live_search_results={}))
            with zip_btn_col:
                st.write("")
                if st.button("Search by ZIP", key="search_by_zip_btn"):
                    st.session_state.live_search_results = {}
                    if zip_search_term:
                        with st.spinner(f"Live searching Fandango for theaters near {zip_search_term}..."):
                            status, result, log = run_async_in_thread(scout.live_search_by_zip, zip_search_term)
                            st.session_state.last_run_log = log
                            if status == 'success':
                                st.session_state.live_search_results = result
                                if not result: st.session_state.show_failsafe = True
                            else: st.error("Failed to perform live ZIP search.")
                    else: st.warning("Please enter a ZIP code.")
        
        if st.session_state.get('show_failsafe', False):
            google_search_query = urllib.parse.quote(f"{name_search_term} fandango")
            google_url = f"https://www.google.com/search?q={google_search_query}"
            st.warning(f"No theaters found for '{name_search_term}'.")
            st.markdown(f"**Failsafe:** [Click here to search Google for the official Fandango page]({google_url}) and paste the URL below.")
            
            url_col, url_btn_col = st.columns([4, 1])
            with url_col:
                pasted_url = st.text_input("Alternatively, paste a direct Fandango Theater URL here", key="pasted_url_input")
            with url_btn_col:
                st.write("")
                if st.button("Add Theater from URL"):
                    if pasted_url and "fandango.com" in pasted_url and "/theater-page" in pasted_url:
                        theater_name_from_url = pasted_url.split("/")[-2].replace("-", " ").title()
                        st.session_state.live_search_results[theater_name_from_url] = {"name": theater_name_from_url, "url": pasted_url}
                        st.session_state.show_failsafe = False
                        st.rerun()
                    else: st.error("Please paste a valid Fandango theater page URL.")

        if st.session_state.live_search_results:
            selected_names = st.multiselect("Select one or more theaters to snipe:",
                options=sorted(st.session_state.live_search_results.keys()), key="compsnipe_selection")
            st.session_state.compsnipe_theaters = [st.session_state.live_search_results[name] for name in selected_names]

        if st.session_state.compsnipe_theaters:
            scrape_date_cs = st.date_input("Select Date for Showtimes", datetime.date.today() + datetime.timedelta(days=1), key="cs_date")
            scrape_date_cs_str = scrape_date_cs.strftime('%Y-%m-%d')
            if st.button("Sniper Report: Get All Showings & Prices", use_container_width=True):
                st.session_state.confirm_scrape = True
                st.rerun()
                
            if st.session_state.get('confirm_scrape'):
                st.info(f"You are about to scrape ALL showings and prices for **{len(st.session_state.compsnipe_theaters)}** selected theater(s) on **{scrape_date_cs_str}**.")
                if st.button("✅ Yes, Proceed with Snipe", use_container_width=True, type="primary"):
                    with st.spinner("Executing sniper scrape... This may take several minutes."):
                        theaters = st.session_state.compsnipe_theaters
                        status_s, showings, log_s = run_async_in_thread(scout.get_all_showings_for_theaters, theaters, scrape_date_cs_str, "CompSnipe Mode")
                        st.session_state.last_run_log += log_s
                        if status_s == 'success':
                            showtimes_for_scrape = {t_name: {} for t_name in showings}
                            for t_name, film_list in showings.items():
                                for film_showing in film_list:
                                    film_title = film_showing['film_title']
                                    if film_title not in showtimes_for_scrape[t_name]: showtimes_for_scrape[t_name][film_title] = {}
                                    showtimes_for_scrape[t_name][film_title][film_showing['showtime']] = film_showing
                            
                            status_p, result, log_p = run_async_in_thread(scout.scrape_details, theaters, showtimes_for_scrape, scrape_date_cs_str)
                            st.session_state.last_run_log += log_p
                            if status_p == 'success' and result:
                                df = pd.DataFrame(result)
                                df['Previous Price'] = 'N/A'
                                df['Price_Change'] = 'N/A'
                                st.session_state.final_df = df
                                report_path = get_report_path(st.session_state.search_mode)
                                timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H%M%S")
                                save_path = os.path.join(report_path, f"compsnipe_report_{timestamp}.csv")
                                st.session_state.final_df.to_csv(save_path, index=False)
                                st.session_state.last_saved_report = save_path
                                st.session_state.stage = 'report_generated'
                            else: st.error("Sniper scrape failed to produce a report.")
                        else: st.error("Failed to fetch showings for CompSnipe mode.")
                    st.session_state.confirm_scrape = False
                    st.rerun()


    # ==============================================================================
    # --- REPORT DISPLAY & GENERATION (Market Mode Continuation) ---
    # ==============================================================================
    if st.session_state.get('confirm_scrape') and st.session_state.search_mode == "Market Mode":
        total_showings = sum(len(times) for films in st.session_state.selected_showtimes.values() for times in films.values())
        total_theaters = len(st.session_state.selected_showtimes)
        st.info(f"You are about to scrape **{total_showings}** showtimes across **{total_theaters}** theaters. This may take a moment.")
        if st.button("✅ Yes, Proceed with Scrape", use_container_width=True, type="primary"):
            with st.spinner("Running detailed scrape..."):
                theaters_for_report = [t for t in st.session_state.theaters if t['name'] in st.session_state.selected_theaters]
                status, result, log = run_async_in_thread(scout.scrape_details, theaters_for_report, st.session_state.selected_showtimes, scrape_date_str)
                st.session_state.last_run_log = log
                st.session_state.confirm_scrape = False
                if status == 'success' and result:
                    df_current = pd.DataFrame(result)
                    df_previous = pd.DataFrame() 
                    if 'selected_report_to_compare' in locals() and selected_report_to_compare != "None":
                        df_previous = pd.read_csv(selected_report_to_compare)
                        merge_keys = ['Theater Name', 'Film Title', 'Showtime', 'Ticket Type']
                        df_merged = pd.merge(df_current, df_previous, on=merge_keys, how='left', suffixes=('_current', '_previous'))
                        df_merged['Price_current_num'] = pd.to_numeric(df_merged['Price_current'].str.replace('$', ''), errors='coerce')
                        df_merged['Price_previous_num'] = pd.to_numeric(df_merged['Price_previous'].str.replace('$', ''), errors='coerce')
                        df_merged['Price_Change'] = df_merged['Price_current_num'] - df_merged['Price_previous_num']
                        df_merged['Price_Change'] = df_merged['Price_Change'].apply(lambda x: f"${x:.2f}" if pd.notna(x) and x > 0 else (f"-${-x:.2f}" if pd.notna(x) and x < 0 else ("$0.00" if pd.notna(x) else "New")))
                        final_cols = ['Theater Name', 'Film Title', 'Showtime', 'Daypart_current', 'Ticket Type', 'Price_current', 'Price_previous', 'Price_Change', 'Capacity_current']
                        df_merged.rename(columns={'Price_current': 'Price', 'Daypart_current': 'Daypart', 'Price_previous': 'Previous Price', 'Capacity_current': 'Capacity'}, inplace=True)
                        st.session_state.final_df = df_merged[[col for col in final_cols if col in df_merged.columns]]
                    else:
                        st.session_state.final_df = df_current
                        st.session_state.final_df['Previous Price'] = 'N/A'
                        st.session_state.final_df['Price_Change'] = 'N/A'
                    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H%M%S")
                    save_path = os.path.join(report_path, f"report_{timestamp}.csv")
                    df_current.to_csv(save_path, index=False)
                    st.session_state.last_saved_report = save_path 
                    st.session_state.stage = 'report_generated'
                else: st.error("Scraper failed to produce a report.")
            st.rerun()

    if st.session_state.get('stage') == 'report_generated' and 'final_df' in st.session_state and not st.session_state.final_df.empty:
        st.header("Live Pricing Report")
        if st.session_state.search_mode == "Market Mode":
            missing_daypart_messages = get_missing_daypart_report(st.session_state.final_df)
            if missing_daypart_messages:
                st.warning("Missing Daypart Selections Found:")
                st.markdown("\n".join(f"- {msg}" for msg in missing_daypart_messages))
        if 'last_saved_report' in st.session_state:
            st.success(f"**Report Complete!** Data has been successfully saved to the following location:")
            st.code(st.session_state.last_saved_report, language='text')
        
        # FINAL FIX: Conditional styling application
        df_to_display = st.session_state.final_df
        if 'Price_Change' in df_to_display.columns:
            st.dataframe(df_to_display.style.applymap(style_price_change, subset=['Price_Change']), use_container_width=True)
        else:
            st.dataframe(df_to_display, use_container_width=True)

        st.balloons()

    if st.session_state.dev_mode and "last_run_log" in st.session_state:
        with st.expander("Developer Mode: Scraper Log"):
            st.code(st.session_state.last_run_log, language='text')