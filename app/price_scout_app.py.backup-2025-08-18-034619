# price_scout_app.py - The Streamlit User Interface & Scraping Engine (v25.0 - Seat Allocation Scraping)

import streamlit as st
import pandas as pd
import os
import datetime
import time
import asyncio
import traceback
import sys
import threading
import urllib.parse
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import re
import json
from functools import reduce
import io
from contextlib import redirect_stdout
import requests # Using requests for fast static page scraping
import random
import sqlite3

# --- Dynamically Define File Paths ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_DIR = os.path.dirname(SCRIPT_DIR)
DEBUG_DIR = os.path.join(PROJECT_DIR, 'debug_snapshots') # Directory for HTML snapshots

# --- Check for Developer Mode ---
query_params = st.query_params
DEV_MODE_ENABLED = query_params.get("dev") == "true"

# --- Streamlit Page Configuration ---
st.set_page_config(
    page_title="PriceScout",
    page_icon=os.path.join(SCRIPT_DIR, 'PriceScoutLogo.png'),
    layout="wide"
)

# --- Custom CSS ---
st.markdown("""
<style>
    /* --- Base Button Style --- */
    button {
        border-radius: 5px !important;
        font-weight: bold !important;
        transition: background-color 0.2s, color 0.2s, border-color 0.2s;
    }
    /* --- Unselected Button --- */
    button.st-emotion-cache-1anq8dj.e7nj0r42 {
        border: 2px solid #A21E25 !important;
        color: #A21E25 !important;
        background-color: transparent !important;
    }
    /* --- Selected Button --- */
    button.st-emotion-cache-1krtkoa.e7nj0r41 {
        background-color: #8B0F05 !important;
        color: white !important;
        border: 1px solid #8B0F05 !important;
    }
    /* --- Hover for All Buttons (Corrected) --- */
    button.st-emotion-cache-1anq8dj.e7nj0r42:hover,
    button.st-emotion-cache-1krtkoa.e7nj0r41:hover {
        background-color: #A21E25 !important;
        color: white !important;
        border-color: #A21E25 !important;
    }
    /* --- Focus Style (Corrected per Option 2) --- */
    button:focus {
        background-color: #8B0F05 !important; /* Dark Red */
        color: white !important;
        border-color: #8B0F05 !important;
        box-shadow: 0 0 0 0.2rem rgba(139, 15, 5, 0.5) !important;
    }

    /* --- Toggle Switch (Final) --- */
    /* Track of the toggle in the OFF state */
    label.st-emotion-cache-1t3w24c input[type="checkbox"] + div.st-emotion-cache-7oyrr6 {
        background-color: #f0f2f6 !important;
        border-color: #A21E25 !important;
    }
    /* Track of the toggle in the ON state */
    label.st-emotion-cache-1t3w24c input[type="checkbox"]:checked + div.st-emotion-cache-7oyrr6 {
        background-color: #A21E25 !important;
        border-color: #A21E25 !important;
    }
    /* Focus style for accessibility */
    label.st-emotion-cache-1t3w24c input[type="checkbox"]:focus + div.st-emotion-cache-7oyrr6 {
        box-shadow: 0 0 0 0.2rem rgba(139, 15, 5, 0.5) !important;
    }
</style>
""", unsafe_allow_html=True)


def check_password():
    """Returns `True` if the user has the correct password."""
    if "password" not in st.secrets:
        return True
    def password_entered():
        if st.session_state["password"] == st.secrets["password"]:
            st.session_state["password_correct"] = True
            del st.session_state["password"]
        else:
            st.session_state["password_correct"] = False
    if st.session_state.get("password_correct", False):
        return True
    st.text_input("Password", type="password", on_change=password_entered, key="password")
    if "password_correct" in st.session_state:
        st.error("üòï Password incorrect")
    return False

# --- Main Application Logic ---
if check_password():
    st.title('üìä PriceScout: Competitive Pricing Tool')

    # --- Constants ---
    DATA_DIR = os.path.join(PROJECT_DIR, 'data', 'Marcus')
    CACHE_FILE = os.path.join(SCRIPT_DIR, 'theater_cache.json')
    MARKETS_FILE = os.path.join(DATA_DIR, 'markets.json')
    CACHE_EXPIRATION_DAYS = 3
    REPORTS_DIR = os.path.join(DATA_DIR, 'reports')
    RUNTIME_LOG_FILE = os.path.join(REPORTS_DIR, 'runtime_log.csv')
    DB_FILE = os.path.join(DATA_DIR, 'price_scout.db')

    # ==============================================================================
    # --- DATABASE SETUP ---
    # ==============================================================================
    def init_database():
        """Initializes the SQLite database and creates/updates tables."""
        with sqlite3.connect(DB_FILE) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS scrape_runs (
                    run_id INTEGER PRIMARY KEY AUTOINCREMENT,
                    run_timestamp DATETIME NOT NULL,
                    mode TEXT NOT NULL
                )
            ''')
            # --- CHANGE START ---: Updated schema for seat allocation
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS prices (
                    price_id INTEGER PRIMARY KEY AUTOINCREMENT,
                    run_id INTEGER,
                    theater_name TEXT NOT NULL,
                    film_title TEXT NOT NULL,
                    showtime TEXT NOT NULL,
                    daypart TEXT,
                    format TEXT,
                    ticket_type TEXT NOT NULL,
                    price REAL NOT NULL,
                    capacity TEXT,
                    seats_total INTEGER,
                    seats_available INTEGER,
                    seats_sold INTEGER,
                    premium_seats_total INTEGER,
                    premium_seats_available INTEGER,
                    premium_seats_sold INTEGER,
                    FOREIGN KEY (run_id) REFERENCES scrape_runs (run_id)
                )
            ''')
            # Add new columns if they don't exist (for backward compatibility)
            table_info = cursor.execute("PRAGMA table_info(prices)").fetchall()
            column_names = [info[1] for info in table_info]
            new_columns = {
                "seats_total": "INTEGER", "seats_available": "INTEGER", "seats_sold": "INTEGER",
                "premium_seats_total": "INTEGER", "premium_seats_available": "INTEGER", "premium_seats_sold": "INTEGER"
            }
            for col, col_type in new_columns.items():
                if col not in column_names:
                    cursor.execute(f"ALTER TABLE prices ADD COLUMN {col} {col_type}")
            # --- CHANGE END ---
            conn.commit()

    def save_to_database(df, mode):
        """Saves a DataFrame of scraped prices to the database."""
        with sqlite3.connect(DB_FILE) as conn:
            cursor = conn.cursor()
            run_timestamp = datetime.datetime.now()
            cursor.execute('INSERT INTO scrape_runs (run_timestamp, mode) VALUES (?, ?)', (run_timestamp, mode))
            run_id = cursor.lastrowid
            
            df_to_save = df.copy()
            df_to_save['run_id'] = run_id
            df_to_save['Price'] = df_to_save['Price'].replace({'\$': ''}, regex=True).astype(float)
            
            # --- CHANGE START ---: Prepare new seat columns for DB
            db_columns = [
                'run_id', 'theater_name', 'film_title', 'showtime', 'daypart', 'format', 
                'ticket_type', 'price', 'capacity', 'seats_total', 'seats_available', 
                'seats_sold', 'premium_seats_total', 'premium_seats_available', 'premium_seats_sold'
            ]
            
            df_to_save = df_to_save.rename(columns={
                'Theater Name': 'theater_name', 'Film Title': 'film_title',
                'Showtime': 'showtime', 'Daypart': 'daypart',
                'Format': 'format', 'Ticket Type': 'ticket_type',
                'Price': 'price', 'Capacity': 'capacity',
                'Seats Total': 'seats_total', 'Seats Available': 'seats_available', 'Seats Sold': 'seats_sold',
                'Premium Seats Total': 'premium_seats_total', 'Premium Seats Available': 'premium_seats_available',
                'Premium Seats Sold': 'premium_seats_sold'
            })
            
            # Ensure all required DB columns exist in the DataFrame, adding them with None if not
            for col in db_columns:
                if col not in df_to_save.columns:
                    df_to_save[col] = None
            
            # Insert the DataFrame into the prices table
            df_to_save.to_sql('prices', conn, if_exists='append', index=False, keys=db_columns)
            # --- CHANGE END ---
            conn.commit()
            print(f"  [DB] Saved {len(df_to_save)} records to database for run ID {run_id}.")


    # --- Data Scraping Engine (Scraper Class) ---
    class Scraper:
        def _sanitize_for_comparison(self, text: str) -> str:
            text = text.lower()
            text = re.sub(r'[^a-z0-9\s]', '', text)
            text = re.sub(r'\s\d+\s', ' ', text)
            text = re.sub(r'\s+', ' ', text).strip()
            return text

        def _sanitize_filename(self, name):
            return re.sub(r'[\\/*?:"<>|]',"", name).replace(" ", "_")

        def _parse_ticket_description(self, description: str) -> dict:
            desc_lower = description.lower()
            amenity_map = {
                'D-BOX': ['d-box', 'dbox'], 'IMAX': ['imax'], 'XD': ['xd'],
                'Dolby Cinema': ['dolby'], 'Recliner': ['recliner'],
                'Luxury': ['luxury'], '4DX': ['4dx'],
                'Promotion': ['promotion', 'tuesday', 'unseen', 'fathom']
            }
            base_type_map = {'Adult': ['adult'], 'Child': ['child'], 'Senior': ['senior'], 'Military': ['military'], 'Student': ['student'], 'Matinee': ['matinee']}
            found_amenities = []
            remaining_desc = desc_lower
            for amenity, keywords in amenity_map.items():
                for keyword in keywords:
                    if keyword in remaining_desc:
                        found_amenities.append(amenity)
                        remaining_desc = remaining_desc.replace(keyword, '').strip()
            found_base_type = None
            for base_type, keywords in base_type_map.items():
                for keyword in keywords:
                    if keyword in remaining_desc:
                        found_base_type = base_type
                        remaining_desc = remaining_desc.replace(keyword, '').strip()
                        break
                if found_base_type: break
            if not found_base_type:
                remaining_desc = description.split('(')[0].strip()
                if remaining_desc.lower() in ['general admission', 'admission']:
                    found_base_type = "General Admission"
                else:
                    found_base_type = remaining_desc
            return {"base_type": found_base_type, "amenities": sorted(list(set(found_amenities)))}

        def _classify_daypart(self, showtime_str: str) -> str:
            try:
                s = showtime_str.strip().lower().replace('.', '')
                if s.endswith('p'): s = s[:-1] + 'pm'
                if s.endswith('a'): s = s[:-1] + 'am'
                s = s.replace('p.m.', 'pm').replace('a.m.', 'am').replace('p ', 'pm').replace('a ', 'am')
                if not any(x in s for x in ('am','pm')):
                    hour_match = re.match(r'(\d{1,2})', s)
                    if hour_match:
                        hour = int(hour_match.group(1))
                        s += 'am' if hour < 8 or hour == 12 else 'pm'
                t = datetime.datetime.strptime(s, "%I:%M%p").time()
                if t < datetime.time(16,0): return "Matinee"
                if t < datetime.time(18,0): return "Twilight"
                if t <= datetime.time(21,0): return "Prime"
                return "Late Night"
            except Exception as e:
                print(f"      [WARNING] Could not classify daypart for '{showtime_str}'. Error: {e}")
                return "Unknown"
        async def _get_theaters_from_zip_page(self, page, zip_code):
            url = f"https://www.fandango.com/{zip_code}_movietimes"
            print(f"  - Checking ZIP: {zip_code}")
            try:
                await page.goto(url, timeout=30000)
                await page.mouse.wheel(0, 2000)
                await page.wait_for_timeout(1500)
                js_condition = "() => window.Fandango && window.Fandango.pageDetails && window.Fandango.pageDetails.localTheaters && window.Fandango.pageDetails.localTheaters.length > 0"
                await page.wait_for_function(js_condition, timeout=20000)
                theaters_data = await page.evaluate('() => window.Fandango.pageDetails.localTheaters')
                return {t.get('name'): {"name": t.get('name'), "url": "https://www.fandango.com" + t.get('theaterPageUrl')} for t in theaters_data if t.get('name') and t.get('theaterPageUrl')}
            except Exception as e:
                print(f"    [WARNING] Could not process ZIP {zip_code}. Error: {e}")
                return {}
        async def live_search_by_zip(self, zip_code):
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                await page.goto(f"https://www.fandango.com/{zip_code}_movietimes", timeout=30000)
                await page.mouse.wheel(0, 2000)
                await page.wait_for_timeout(1500)
                js_condition = "() => window.Fandango && window.Fandango.pageDetails && window.Fandango.pageDetails.localTheaters && window.Fandango.pageDetails.localTheaters.length > 0"
                await page.wait_for_function(js_condition, timeout=20000)
                theaters_data = await page.evaluate('() => window.Fandango.pageDetails.localTheaters')
                results = {t.get('name'): {"name": t.get('name'), "url": "https://www.fandango.com" + t.get('theaterPageUrl')} for t in theaters_data if t.get('name') and t.get('theaterPageUrl')}
                await browser.close()
                return results

        async def live_search_by_name(self, search_term):
            print(f"  - Live searching for: {search_term}")
            results = {}
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                try:
                    await page.goto("https://www.fandango.com", timeout=60000)
                    await page.locator('[data-qa="search-input"]').fill(search_term)
                    await page.locator('[data-qa="search-input"]').press('Enter')
                    await page.wait_for_selector('[data-qa="search-results-item"]', timeout=15000)
                    soup = BeautifulSoup(await page.content(), 'html.parser')
                    search_results_items = soup.select('[data-qa="search-results-item"]')
                    for item in search_results_items:
                        link_elem = item.select_one('a[data-qa="search-results-item-link"]')
                        if link_elem and '/theater-page' in link_elem.get('href', ''):
                            name = link_elem.get_text(strip=True)
                            url = "https://www.fandango.com" + link_elem['href']
                            results[name] = {"name": name, "url": url}
                except Exception as e:
                    print(f"    [WARNING] Could not complete live name search. Error: {e}")
                await browser.close()
                return results

        async def build_theater_cache(self, markets_json_path):
            with open(markets_json_path, 'r') as f:
                markets_data = json.load(f)

            temp_cache = {"metadata": {"last_updated": datetime.datetime.now().isoformat()}, "markets": {}}
            total_theaters_to_find = 0
            total_theaters_found = 0

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()

                for parent_company, regions in markets_data.items():
                    for region_name, markets in regions.items():
                        for market_name, market_info in markets.items():
                            print(f"\n--- Processing Market: {market_name} ---")
                            theaters_in_market = market_info.get('theaters', [])
                            total_theaters_to_find += len(theaters_in_market)

                            found_theaters_for_market = []
                            theaters_to_find_in_fallback = []

                            print("  [Phase 1] Starting fast ZIP code scrape...")
                            zip_pool = {t.get('zip') for t in theaters_in_market if t.get('zip')}
                            market_zip_cache = {}
                            for zip_code in zip_pool:
                                zip_results = await self._get_theaters_from_zip_page(page, zip_code)
                                market_zip_cache.update(zip_results)

                            for theater_from_json in theaters_in_market:
                                name_to_find = theater_from_json['name']
                                found = False
                                sanitized_target_name = self._sanitize_for_comparison(name_to_find)
                                for live_name, live_data in market_zip_cache.items():
                                    sanitized_live_name = self._sanitize_for_comparison(live_name)
                                    if sanitized_target_name in sanitized_live_name or sanitized_live_name in sanitized_target_name:
                                        found_theaters_for_market.append({'name': live_name, 'url': live_data['url']})
                                        found = True
                                        break
                                if not found:
                                    theaters_to_find_in_fallback.append(name_to_find)

                            print(f"  [Phase 1] Found {len(found_theaters_for_market)} theaters via ZIP scrape.")

                            if theaters_to_find_in_fallback:
                                print(f"  [Phase 2] Starting targeted fallback search for {len(theaters_to_find_in_fallback)} theater(s)...")
                                for theater_name in theaters_to_find_in_fallback:
                                    search_results = await self.live_search_by_name(theater_name)
                                    if search_results:
                                        found_name, found_data = next(iter(search_results.items()))
                                        print(f"    [SUCCESS] Fallback found '{theater_name}' as '{found_name}'")
                                        found_theaters_for_market.append({'name': found_name, 'url': found_data['url']})
                                    else:
                                        print(f"    [WARNING] Fallback could not find '{theater_name}'.")

                            temp_cache["markets"][market_name] = {"theaters": found_theaters_for_market}
                            total_theaters_found += len(found_theaters_for_market)

                await browser.close()

            print("\n--- Sanity Check ---")
            print(f"Found {total_theaters_found} out of {total_theaters_to_find} total theaters.")

            if total_theaters_to_find > 0 and (total_theaters_found / total_theaters_to_find) >= 0.75:
                print("[SUCCESS] Sanity check passed. Overwriting old cache.")
                with open(CACHE_FILE, 'w') as f:
                    json.dump(temp_cache, f, indent=2)
                return temp_cache
            else:
                print("[FAILURE] Sanity check failed. Preserving existing cache to prevent errors.")
                return False

        async def _get_movies_from_theater_page(self, page, theater, date):
            full_url = f"{theater['url']}?date={date}"
            html_content = ""
            try:
                await page.goto(full_url, timeout=60000)
                await page.locator('div.theater-presenting-formats, li.fd-panel').first.wait_for(timeout=30000)
                html_content = await page.content()
                soup = BeautifulSoup(html_content, 'html.parser')
                showings = []
                movie_blocks = soup.select('li.fd-panel')
                if not movie_blocks and st.session_state.get('capture_html', False):
                    os.makedirs(DEBUG_DIR, exist_ok=True)
                    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"debug_{self._sanitize_filename(theater['name'])}_{timestamp}.html"
                    filepath = os.path.join(DEBUG_DIR, filename)
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(html_content)
                    print(f"  [DEBUG] No films found for {theater['name']}. Saved HTML snapshot to {filepath}")
                for movie_block in movie_blocks:
                    film_title_elem = movie_block.select_one('h2.thtr-mv-list__detail-title a')
                    film_title = film_title_elem.get_text(strip=True) if film_title_elem else "Unknown Title"
                    variant_title_elem = movie_block.select_one('.movie-variant-title')
                    variant_title = variant_title_elem.get_text(strip=True) if variant_title_elem else None
                    showtime_links = movie_block.select('ol.showtimes-btn-list a.showtime-btn')
                    for link in showtime_links:
                        time_label_elem = link.select_one('.showtime-btn-label')
                        amenity_elem = link.select_one('.showtime-btn-amenity')
                        time_str = time_label_elem.get_text(strip=True) if time_label_elem else link.get_text(strip=True)
                        amenity_str = amenity_elem.get_text(strip=True) if amenity_elem else variant_title
                        movie_format = amenity_str if amenity_str else "2D"
                        ticket_url = "https://tickets.fandango.com/transaction/ticketing/mobile/jump.aspx" + link.get('href', '').split('jump.aspx')[-1]
                        if film_title != "Unknown Title" and time_str and ticket_url and re.match(r'\d{1,2}:\d{2}[ap]m?', time_str, re.IGNORECASE):
                            showings.append({"film_title": film_title, "format": movie_format, "showtime": time_str, "daypart": self._classify_daypart(time_str), "ticket_url": ticket_url})
                return showings
            except Exception as e:
                print(f"    [ERROR] Failed to get movies for {theater['name']}. Error: {e}")
                return []

        # --- CHANGE START ---: Major enhancement to scrape seat allocation
        async def _get_prices_and_capacity(self, page, showing_details):
            showtime_url = showing_details['ticket_url']
            results = {"tickets": [], "error": None}
            # Initialize detailed seat counts
            seat_counts = {
                'seats_total': 0, 'seats_available': 0, 'seats_sold': 0,
                'premium_seats_total': 0, 'premium_seats_available': 0, 'premium_seats_sold': 0
            }
            results.update(seat_counts)

            try:
                await page.goto(showtime_url, timeout=60000)
                await page.wait_for_timeout(random.randint(1500, 2500))
                
                scripts = await page.query_selector_all('script')
                for script in scripts:
                    content = await script.inner_html()
                    if content and 'window.Commerce.models' in content:
                        start_text = 'window.Commerce.models = '
                        start_index = content.find(start_text)
                        if start_index != -1:
                            json_start = content.find('{', start_index)
                            open_braces, json_end = 0, -1
                            for i in range(json_start, len(content)):
                                if content[i] == '{': open_braces += 1
                                elif content[i] == '}': open_braces -= 1
                                if open_braces == 0:
                                    json_end = i + 1; break
                            if json_end != -1:
                                data = json.loads(content[json_start:json_end])
                                
                                # 1. Scrape Ticket Prices (existing logic)
                                ticket_types = data.get('tickets', {}).get('seatingAreas', [{}])[0].get('ticketTypes', [])
                                for tt in ticket_types:
                                    description, price = tt.get('description'), tt.get('price')
                                    if description and price is not None:
                                        parsed_ticket = self._parse_ticket_description(description)
                                        results["tickets"].append({
                                            "type": parsed_ticket["base_type"],
                                            "price": f"${price:.2f}",
                                            "amenities": parsed_ticket["amenities"]
                                        })

                                # 2. Scrape Seat Allocation (new logic)
                                seat_map = data.get('seating', {}).get('seatMap', {})
                                if seat_map:
                                    for row in seat_map.get('rows', []):
                                        for seat in row.get('seats', []):
                                            is_premium = 'd-box' in seat.get('type', '').lower()
                                            status = seat.get('status')

                                            if status == 'Available':
                                                if is_premium:
                                                    seat_counts['premium_seats_available'] += 1
                                                else:
                                                    seat_counts['seats_available'] += 1
                                            elif status == 'Sold':
                                                if is_premium:
                                                    seat_counts['premium_seats_sold'] += 1
                                                else:
                                                    seat_counts['seats_sold'] += 1
                                
                                    # Calculate totals
                                    seat_counts['seats_total'] = seat_counts['seats_available'] + seat_counts['seats_sold']
                                    seat_counts['premium_seats_total'] = seat_counts['premium_seats_available'] + seat_counts['premium_seats_sold']
                                    results.update(seat_counts)
                                    results['capacity'] = f"{seat_counts['seats_available']} / {seat_counts['seats_total']}"

                                if results["tickets"]: return results
            except Exception as e:
                results["error"] = f'Scraping failed: {e}'
            return results
        # --- CHANGE END ---

        async def get_all_showings_for_theaters(self, theaters, date):
            # ... (unchanged)
            showings_by_theater = {}
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                for theater in theaters:
                    context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
                    page = await context.new_page()
                    showings = await self._get_movies_from_theater_page(page, theater, date)
                    showings_by_theater[theater['name']] = showings
                    await context.close()
            return showings_by_theater

        async def scrape_details(self, theaters, selected_showtimes):
            # ... (unchanged, but now returns more data from _get_prices_and_capacity)
            all_price_data = []
            showings_to_scrape = []
            for theater in theaters:
                if theater['name'] in selected_showtimes:
                    for film, times in selected_showtimes[theater['name']].items():
                        for time_str, showing_info in times.items():
                            showings_to_scrape.append({**showing_info, "theater_name": theater['name']})

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                for showing in showings_to_scrape:
                    context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
                    page = await context.new_page()
                    scrape_results = await self._get_prices_and_capacity(page, showing)
                    await context.close()

                    if scrape_results["error"]:
                        print(f"  [ERROR] Scraping {showing['film_title']} at {showing['theater_name']}: {scrape_results['error']}")
                        continue
                    
                    # Combine ticket data with the overall seat allocation data
                    for ticket in scrape_results['tickets']:
                        initial_format = showing['format']
                        final_amenities = ticket['amenities']
                        combined_format_list = [initial_format] + final_amenities
                        unique_formats = sorted(list(set(combined_format_list)))
                        if len(unique_formats) > 1 and "2D" in unique_formats:
                            unique_formats.remove("2D")
                        
                        price_point = {
                            "Theater Name": showing['theater_name'], "Film Title": showing['film_title'],
                            "Format": ", ".join(unique_formats), "Showtime": showing['showtime'],
                            "Daypart": showing['daypart'], "Ticket Type": ticket['type'], "Price": ticket['price'],
                            "Capacity": scrape_results.get('capacity', 'N/A'),
                            "Seats Total": scrape_results.get('seats_total'),
                            "Seats Available": scrape_results.get('seats_available'),
                            "Seats Sold": scrape_results.get('seats_sold'),
                            "Premium Seats Total": scrape_results.get('premium_seats_total'),
                            "Premium Seats Available": scrape_results.get('premium_seats_available'),
                            "Premium Seats Sold": scrape_results.get('premium_seats_sold')
                        }
                        all_price_data.append(price_point)

            return all_price_data, showings_to_scrape

        async def run_diagnostic_scrape(self, markets_to_test, date):
            # ... (unchanged)
            diagnostic_results = []
            theaters_to_test = []
            with open(CACHE_FILE, 'r') as f:
                cache = json.load(f)
            for market_name in markets_to_test:
                theaters = cache.get("markets", {}).get(market_name, {}).get("theaters", [])
                for theater in theaters:
                    theater['market'] = market_name
                    theaters_to_test.append(theater)

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                for i, theater in enumerate(theaters_to_test):
                    print(f"Testing {i+1}/{len(theaters_to_test)}: {theater['name']}")
                    context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
                    page = await context.new_page()
                    result_row = {"Market": theater['market'], "Theater Name": theater['name'], "Status": "Failed", "Details": "No showtimes found", "Sample Price": "N/A"}
                    try:
                        showings = await self._get_movies_from_theater_page(page, theater, date)
                        if showings:
                            first_showing = showings[0]
                            price_results = await self._get_prices_and_capacity(page, first_showing)
                            if price_results['tickets']:
                                first_ticket = price_results['tickets'][0]
                                result_row.update({
                                    "Status": "Success",
                                    "Details": f"Scraped '{first_showing['film_title']}' at {first_showing['showtime']}",
                                    "Sample Price": f"{first_ticket['type']}: {first_ticket['price']}"
                                })
                            else:
                                result_row["Details"] = "Failed to extract price from ticket page."
                        diagnostic_results.append(result_row)
                    except Exception as e:
                        result_row["Details"] = f"An unexpected error occurred: {str(e)}"
                        diagnostic_results.append(result_row)
                    finally:
                        await context.close()
            return diagnostic_results

    scout = Scraper()

    # --- UI Helper Functions ---
    # ... (All helper functions like run_async_in_thread, check_cache_status, etc., are unchanged)
    def run_async_in_thread(target_func, *args):
        log_stream = io.StringIO()
        status, value, duration = 'error', None, 0.0

        def thread_target():
            nonlocal status, value, duration
            start_time = time.time()
            if sys.platform == "win32":
                asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
            try:
                with redirect_stdout(log_stream):
                    if len(args) == 1 and asyncio.iscoroutine(args[0]):
                         result = asyncio.run(args[0])
                    else:
                         result = asyncio.run(target_func(*args))
                duration = time.time() - start_time
                status, value = 'success', result
            except Exception:
                duration = time.time() - start_time
                error_str = traceback.format_exc()
                print(f"\n--- TRACEBACK ---\n{error_str}", file=log_stream)
                status, value = 'error', error_str

        thread = threading.Thread(target=thread_target)
        thread.start()
        thread.join()
        log_output = log_stream.getvalue()
        return status, value, log_output, duration

    def check_cache_status():
        if not os.path.exists(CACHE_FILE):
            return "missing", None
        try:
            with open(CACHE_FILE, 'r') as f:
                cache_data = json.load(f)
            last_updated_str = cache_data.get("metadata", {}).get("last_updated")
            if not last_updated_str:
                return "invalid", None
            last_updated = datetime.datetime.fromisoformat(last_updated_str)
            if (datetime.datetime.now() - last_updated).days >= CACHE_EXPIRATION_DAYS:
                return "stale", last_updated.strftime('%Y-%m-%d %H:%M')
            return "fresh", last_updated.strftime('%Y-%m-%d %H:%M')
        except (json.JSONDecodeError, KeyError):
            return "invalid", None

    def get_report_path(mode, region=None, market=None):
        if mode == "Market Mode":
            path = os.path.join(REPORTS_DIR, "MarketMode", scout._sanitize_filename(region or ""), scout._sanitize_filename(market or ""))
        elif mode == "CompSnipe Mode":
            path = os.path.join(REPORTS_DIR, "SnipeMode")
        else:
            path = os.path.join(REPORTS_DIR, "misc")
        os.makedirs(path, exist_ok=True)
        return path

    def log_runtime(mode, theater_count, showtime_count, duration):
        os.makedirs(REPORTS_DIR, exist_ok=True)
        file_exists = os.path.isfile(RUNTIME_LOG_FILE)
        with open(RUNTIME_LOG_FILE, 'a', newline='') as f:
            writer = pd.DataFrame([{
                'timestamp': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'mode': mode,
                'theater_count': theater_count,
                'showtime_count': showtime_count,
                'duration_seconds': round(duration, 2)
            }])
            writer.to_csv(f, header=not file_exists, index=False)

    def clear_workflow_state():
        """Clears session state related to a report workflow, but preserves the mode."""
        keys_to_clear = [
            'stage', 'selected_region', 'selected_market', 'theaters',
            'selected_theaters', 'all_showings', 'selected_films',
            'selected_showtimes', 'confirm_scrape', 'compsnipe_theaters',
            'live_search_results', 'daypart_selections',
            'compsnipe_film_filter_mode', 'market_films_filter'
        ]
        for key in keys_to_clear:
            if key in st.session_state:
                del st.session_state[key]

    def reset_session():
        """Resets the entire session state to its initial default."""
        st.session_state.search_mode = "Market Mode"
        keys_to_reset = [
            'stage', 'selected_region', 'selected_market', 'theaters',
            'selected_theaters', 'all_showings', 'selected_films',
            'selected_showtimes', 'confirm_scrape', 'compsnipe_theaters',
            'live_search_results', 'last_mode', 'run_diagnostic', 'last_run_duration',
            'daypart_selections', 'compsnipe_film_filter_mode', 'market_films_filter'
        ]
        for key in keys_to_reset:
            if key in st.session_state:
                del st.session_state[key]
        if 'final_df' in st.session_state:
            del st.session_state.final_df
        st.rerun()

    def style_price_change(val):
        if isinstance(val, str):
            if val.startswith('-$'):
                return 'color: green; font-weight: bold;'
            elif val.startswith('$') and val != '$0.00':
                return 'color: red; font-weight: bold;'
        return ''
    
    @st.cache_data
    def to_excel(df):
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='PriceScout_Report')
        processed_data = output.getvalue()
        return processed_data

    @st.cache_data
    def to_csv(df):
        return df.to_csv(index=False).encode('utf-8')

    # --- Session State Initialization ---
    if 'stage' not in st.session_state: st.session_state.stage = 'initial'
    if 'search_mode' not in st.session_state: st.session_state.search_mode = "Market Mode"
    if 'last_run_log' not in st.session_state: st.session_state.last_run_log = ""
    if 'dev_mode' not in st.session_state: st.session_state.dev_mode = False
    if 'capture_html' not in st.session_state: st.session_state.capture_html = False
    if 'confirm_scrape' not in st.session_state: st.session_state.confirm_scrape = False
    if 'live_search_results' not in st.session_state: st.session_state.live_search_results = {}
    if 'report_running' not in st.session_state: st.session_state.report_running = False
    if 'all_showings' not in st.session_state: st.session_state.all_showings = {}
    if 'selected_films' not in st.session_state: st.session_state.selected_films = []
    if 'selected_showtimes' not in st.session_state: st.session_state.selected_showtimes = {}
    if 'daypart_selections' not in st.session_state: st.session_state.daypart_selections = []


    # --- Load initial data & DB ---
    init_database()
    try:
        with open(MARKETS_FILE, 'r') as f:
            markets_data = json.load(f)
        with open(CACHE_FILE, 'r') as f:
            cache_data = json.load(f)
    except FileNotFoundError:
        st.error(f"Error: `{MARKETS_FILE}` or `{CACHE_FILE}` not found. Please ensure they are in the same directory.")
        st.info("You may need to build the theater cache first if it's missing.")
        st.stop()

    IS_DISABLED = st.session_state.report_running

    # --- Main UI ---
    st.sidebar.title("Controls")
    st.sidebar.image(os.path.join(SCRIPT_DIR, 'PriceScoutLogo.png'))
    if st.sidebar.button("üöÄ Start New Report / Abort", use_container_width=True):
        if 'report_running' in st.session_state:
            st.session_state.report_running = False
        reset_session()

    st.sidebar.divider()

    if DEV_MODE_ENABLED:
        st.sidebar.header("Developer Tools")
        st.session_state.dev_mode = True
        st.session_state.capture_html = st.sidebar.toggle("Capture HTML Snapshots", help="Save HTML files for analysis.", disabled=IS_DISABLED)
        if st.sidebar.button("Run Full System Diagnostic", use_container_width=True, disabled=IS_DISABLED):
            st.session_state.run_diagnostic = True
            st.rerun()
    else:
        st.session_state.dev_mode = False

    if st.session_state.get('run_diagnostic'):
        st.header("üõ†Ô∏è Full System Diagnostic")
        st.warning("**Warning:** This will scrape every theater in the selected markets and may take a very long time to complete.")
        all_markets = list(markets_data["Marcus Theatres"]["South MT"].keys()) + list(markets_data["Marcus Theatres"]["North MT"].keys())
        markets_to_test = st.multiselect("Select markets to test:", options=all_markets, default=all_markets, disabled=IS_DISABLED)
        if st.button("Start Full Diagnostic", type="primary", disabled=IS_DISABLED):
            st.session_state.report_running = True
            st.rerun()

    if st.session_state.report_running and st.session_state.get('run_diagnostic'):
         with st.spinner("Running diagnostic scan... This will take a long time."):
            diag_date_str = (datetime.date.today() + datetime.timedelta(days=1)).strftime('%Y-%m-%d')
            status, result, log, duration = run_async_in_thread(scout.run_diagnostic_scrape, markets_to_test, diag_date_str)
            st.session_state.last_run_log += log
            if status == 'success':
                st.success(f"Diagnostic Complete! (Took {duration:.2f} seconds)")
                df_diag = pd.DataFrame(result)
                st.dataframe(df_diag)
            else:
                st.error(f"The diagnostic tool encountered a critical error after {duration:.2f} seconds.")
            st.session_state.report_running = False
            st.session_state.run_diagnostic = False
            st.rerun()


    st.subheader("Theater Data Cache")
    cache_status, last_updated = check_cache_status()
    if cache_status == "fresh":
        st.success(f"Theater cache is up to date. Last refreshed: {last_updated}")
    elif cache_status == "stale":
        st.warning(f"Theater cache is stale (older than {CACHE_EXPIRATION_DAYS} days). Last refreshed: {last_updated}")
    else:
        st.error("Theater cache file is missing or invalid. Please build it.")

    if st.button("Build / Refresh Theater Cache", disabled=IS_DISABLED):
        with st.spinner("Building theater cache... grab a coffee!"):
            status, result, log, duration = run_async_in_thread(scout.build_theater_cache, MARKETS_FILE)
            st.session_state.last_run_log = log
            if status == 'success':
                st.success(f"Theater cache built successfully in {duration:.2f} seconds!")
                st.rerun()
            else:
                st.error(f"Failed to build theater cache after {duration:.2f} seconds.")
    st.divider()

    st.subheader("Step 1: Define Search Area")
    cols_mode = st.columns(2)
    is_market_mode = st.session_state.search_mode == "Market Mode"

    if cols_mode[0].button("Market Mode", use_container_width=True, type="primary" if is_market_mode else "secondary", disabled=IS_DISABLED):
        if not is_market_mode:
            clear_workflow_state()
            st.session_state.search_mode = "Market Mode"
            st.rerun()

    if cols_mode[1].button("CompSnipe Mode", use_container_width=True, type="primary" if not is_market_mode else "secondary", disabled=IS_DISABLED):
        if is_market_mode:
            clear_workflow_state()
            st.session_state.search_mode = "CompSnipe Mode"
            st.rerun()
        if is_market_mode:
            st.session_state.search_mode = "CompSnipe Mode"
            st.rerun()
    # --- CHANGE END ---

    # ==============================================================================
    # --- DAYPART AUTO-SELECTION LOGIC ---
    # ==============================================================================
    def handle_daypart_click(daypart, all_showings, films_to_process, theaters_to_process):
        """Toggles a single daypart and applies the selection logic."""
        current_selection = st.session_state.daypart_selections
        
        # Logic to handle "All" vs other dayparts
        if daypart == "All":
            if "All" in current_selection:
                current_selection = []
            else:
                current_selection = ["All"]
        else:
            if "All" in current_selection:
                current_selection.remove("All")
            if daypart in current_selection:
                current_selection.remove(daypart)
            else:
                current_selection.append(daypart)
        
        st.session_state.daypart_selections = current_selection
        apply_daypart_auto_selection(current_selection, all_showings, films_to_process, theaters_to_process)

    def apply_daypart_auto_selection(daypart_selections, all_showings, films_to_process, theaters_to_process):
        st.session_state.selected_showtimes = {}
        if not daypart_selections:
            return

        for theater_name in theaters_to_process:
            st.session_state.selected_showtimes[theater_name] = {}
            for film_title in films_to_process:
                st.session_state.selected_showtimes[theater_name][film_title] = {}
                
                showings_for_film = [s for s in all_showings.get(theater_name, []) if s['film_title'] == film_title]
                if not showings_for_film:
                    continue

                selected_dayparts = set(daypart_selections)
                sorted_showings = sorted(showings_for_film, key=lambda x: datetime.datetime.strptime(x['showtime'].replace('p', 'PM').replace('a', 'AM'), "%I:%M%p").time())
                
                if "All" in selected_dayparts:
                    showing = sorted_showings[0]
                    st.session_state.selected_showtimes[theater_name][film_title][showing['showtime']] = showing
                    continue
                
                found_showings_for_dayparts = {}
                for showing in sorted_showings:
                    daypart = showing['daypart']
                    if daypart in selected_dayparts and daypart not in found_showings_for_dayparts:
                        found_showings_for_dayparts[daypart] = showing
                
                for showing in found_showings_for_dayparts.values():
                     st.session_state.selected_showtimes[theater_name][film_title][showing['showtime']] = showing

    # ==============================================================================
    # --- MARKET MODE ---
    # ==============================================================================
    if st.session_state.search_mode == "Market Mode":
        if 'selected_region' not in st.session_state: st.session_state.selected_region = None
        if 'selected_market' not in st.session_state: st.session_state.selected_market = None

        parent_company = list(markets_data.keys())[0]
        regions = list(markets_data[parent_company].keys())
        cols = st.columns(len(regions))
        for i, region in enumerate(regions):
            is_selected = st.session_state.selected_region == region
            if cols[i].button(region, key=f"region_{region}", type="primary" if is_selected else "secondary", use_container_width=True, disabled=IS_DISABLED):
                st.session_state.selected_region = region
                st.session_state.selected_market = None
                st.session_state.stage = 'region_selected'
                st.rerun()

        if st.session_state.selected_region:
            st.markdown(f"### Region: {st.session_state.selected_region}")
            markets = list(markets_data[parent_company][st.session_state.selected_region].keys())
            st.write("Select a Market:")
            market_cols = st.columns(4)
            for i, market in enumerate(markets):
                is_selected = st.session_state.selected_market == market
                if market_cols[i % 4].button(market, key=f"market_{market}", type="primary" if is_selected else "secondary", use_container_width=True, disabled=IS_DISABLED):
                    st.session_state.selected_market = market
                    st.session_state.theaters = cache_data.get("markets", {}).get(market, {}).get("theaters", [])
                    st.session_state.selected_theaters = [t['name'] for t in st.session_state.theaters]
                    st.session_state.stage = 'theaters_listed'
                    st.rerun()

        if st.session_state.stage in ['theaters_listed', 'data_fetched', 'report_generated']:
            st.subheader("Step 2: Select Theaters & Options")
            cols = st.columns(4)
            theaters = st.session_state.get('theaters', [])
            for i, theater in enumerate(theaters):
                is_selected = theater['name'] in st.session_state.get('selected_theaters', [])
                if cols[i % 4].button(theater['name'], key=f"theater_{i}", type="primary" if is_selected else "secondary", use_container_width=True, disabled=IS_DISABLED):
                    if 'selected_theaters' not in st.session_state: st.session_state.selected_theaters = []
                    if is_selected: st.session_state.selected_theaters.remove(theater['name'])
                    else: st.session_state.selected_theaters.append(theater['name'])
                    st.rerun()
            
            st.toggle("Only show films playing at ALL selected theaters", key="market_films_filter", disabled=IS_DISABLED, help="Filters the list in Step 3 to only show films common to every theater selected.")

            scrape_date = st.date_input("Select Date for Showtimes", datetime.date.today() + datetime.timedelta(days=1), key="market_date", disabled=IS_DISABLED)
            scrape_date_str = scrape_date.strftime('%Y-%m-%d')

            if st.button("Find Films for Selected Theaters", disabled=IS_DISABLED, use_container_width=True):
                theaters_to_scrape = [t for t in theaters if t['name'] in st.session_state.selected_theaters]
                with st.spinner("Finding all available films and showtimes..."):
                    status, result, log, duration = run_async_in_thread(scout.get_all_showings_for_theaters, theaters_to_scrape, scrape_date_str)
                    st.session_state.last_run_log = log
                    if status == 'success':
                        st.info(f"Film search completed in {duration:.2f} seconds.")
                        st.session_state.all_showings = result
                        st.session_state.selected_films = []
                        st.session_state.selected_showtimes = {}
                        st.session_state.stage = 'data_fetched'
                    else: st.error("Failed to fetch showings for theaters.")
                st.rerun()

        if st.session_state.stage in ['data_fetched', 'report_generated']:
            st.subheader("Step 3: Select Films & Showtimes")
            
            all_films_unfiltered = sorted(list(reduce(lambda a, b: a.union(b), [set(s['film_title'] for s in showings) for showings in st.session_state.all_showings.values() if showings], set())))
            
            if st.session_state.get('market_films_filter'):
                film_sets = [set(s['film_title'] for s in st.session_state.all_showings.get(theater, [])) for theater in st.session_state.selected_theaters]
                if film_sets:
                    common_films = set.intersection(*film_sets)
                    all_films_to_display = sorted(list(common_films))
                else:
                    all_films_to_display = []
            else:
                all_films_to_display = all_films_unfiltered

            st.write("Select Films:")
            cols = st.columns(4)
            for i, film in enumerate(all_films_to_display):
                is_selected = film in st.session_state.selected_films
                if cols[i % 4].button(film, key=f"film_{film}", type="primary" if is_selected else "secondary", use_container_width=True, disabled=IS_DISABLED):
                    if is_selected: st.session_state.selected_films.remove(film)
                    else: st.session_state.selected_films.append(film)
                    st.rerun()
            st.divider()

            if st.session_state.selected_films:
                 st.write("Auto-select showtimes by Daypart:")
                 daypart_cols = st.columns(5)
                 dayparts = ["All", "Matinee", "Twilight", "Prime", "Late Night"]
                 for i, dp in enumerate(dayparts):
                     is_selected = dp in st.session_state.daypart_selections
                     if daypart_cols[i].button(dp, key=f"market_dp_{dp}", type="primary" if is_selected else "secondary", use_container_width=True, disabled=IS_DISABLED):
                         handle_daypart_click(dp, st.session_state.all_showings, st.session_state.selected_films, st.session_state.selected_theaters)
                         st.rerun()

            for theater_name in st.session_state.get('selected_theaters', []):
                has_selections = any(st.session_state.selected_showtimes.get(theater_name, {}).values())
                expander_label = f"‚úÖ  {theater_name}" if has_selections else f"‚ö™Ô∏è {theater_name}"
                with st.expander(expander_label, expanded=True):
                    showings = st.session_state.all_showings.get(theater_name, [])
                    films_to_display = {f for f in st.session_state.selected_films if f in [s['film_title'] for s in showings]}
                    if not films_to_display: st.write("No selected films are showing at this theater.")
                    for film in sorted(list(films_to_display)):
                        st.markdown(f"**{film}**")
                        film_showings = sorted([s for s in showings if s['film_title'] == film], key=lambda x: datetime.datetime.strptime(x['showtime'].replace('p', 'PM').replace('a', 'AM'), "%I:%M%p").time())
                        cols = st.columns(8)
                        for i, showing in enumerate(film_showings):
                            time_str = showing['showtime']
                            is_selected = time_str in st.session_state.selected_showtimes.get(theater_name, {}).get(film, {})
                            if cols[i % 8].button(time_str, key=f"time_{theater_name}_{film}_{i}", type="primary" if is_selected else "secondary", use_container_width=True, disabled=IS_DISABLED):
                                if theater_name not in st.session_state.selected_showtimes: st.session_state.selected_showtimes[theater_name] = {}
                                if film not in st.session_state.selected_showtimes[theater_name]: st.session_state.selected_showtimes[theater_name][film] = {}
                                if is_selected: del st.session_state.selected_showtimes[theater_name][film][time_str]
                                else: st.session_state.selected_showtimes[theater_name][film][time_str] = showing
                                st.rerun()

        if any(any(film.values()) for film in st.session_state.get('selected_showtimes', {}).values()):
            st.subheader("Step 4: Generate Report")
            previous_reports = []
            report_path = get_report_path("Market Mode", st.session_state.get('selected_region'), st.session_state.get('selected_market'))
            if report_path and os.path.exists(report_path):
                previous_reports = sorted([os.path.join(report_path, f) for f in os.listdir(report_path) if f.endswith('.csv')], reverse=True)
            st.selectbox("Compare to Previous Report (Optional):", ["None"] + previous_reports, help="Select a previous report to compare prices against.", disabled=IS_DISABLED, key="selected_report_to_compare")
            if st.button('üìÑ Generate Live Pricing Report', use_container_width=True, disabled=IS_DISABLED):
                st.session_state.confirm_scrape = True
                st.rerun()

    # ==============================================================================
    # --- COMPSNIPE MODE ---
    # ==============================================================================
    elif st.session_state.search_mode == "CompSnipe Mode":
        st.info("Use this mode for targeted analysis of individual theaters by searching for competitors within a ZIP code.")
        
        st.subheader("Step 1: Select Theaters")
        zip_col, zip_btn_col = st.columns([4, 1])
        with zip_col:
            zip_search_term = st.text_input("Enter 5-digit ZIP code to find theaters", max_chars=5, key="zip_search_input",
                                            on_change=lambda: st.session_state.update(live_search_results={}, compsnipe_theaters=[]), disabled=IS_DISABLED)
        with zip_btn_col:
            st.write("") 
            if st.button("Search by ZIP", key="search_by_zip_btn", disabled=IS_DISABLED):
                with st.spinner(f"Live searching Fandango for theaters near {zip_search_term}..."):
                    status, result, log, _ = run_async_in_thread(scout.live_search_by_zip, zip_search_term)
                    st.session_state.last_run_log = log
                    if status == 'success': st.session_state.live_search_results = result
                    else: st.error("Failed to perform live ZIP search.")
                st.rerun()

        if st.session_state.live_search_results:
            cols = st.columns(4)
            for i, name in enumerate(sorted(st.session_state.live_search_results.keys())):
                if 'compsnipe_theaters' not in st.session_state: st.session_state.compsnipe_theaters = []
                is_selected = name in [t['name'] for t in st.session_state.compsnipe_theaters]
                if cols[i % 4].button(name, key=f"cs_theater_{i}", type="primary" if is_selected else "secondary", use_container_width=True, disabled=IS_DISABLED):
                    theater_obj = st.session_state.live_search_results[name]
                    if is_selected:
                        st.session_state.compsnipe_theaters = [t for t in st.session_state.compsnipe_theaters if t['name'] != name]
                    else:
                        st.session_state.compsnipe_theaters.append(theater_obj)
                    st.rerun()

        if st.session_state.get('compsnipe_theaters'):
            scrape_date_cs = st.date_input("Select Date for Showtimes", datetime.date.today() + datetime.timedelta(days=1), key="cs_date", disabled=IS_DISABLED)
            
            if st.button("Find Available Films", use_container_width=True, disabled=IS_DISABLED):
                with st.spinner("Finding all available films and showtimes..."):
                    status, result, log, duration = run_async_in_thread(scout.get_all_showings_for_theaters, st.session_state.compsnipe_theaters, scrape_date_cs.strftime('%Y-%m-%d'))
                    st.session_state.last_run_log = log
                    if status == 'success':
                        st.info(f"Film search completed in {duration:.2f} seconds.")
                        st.session_state.all_showings = result
                        st.session_state.stage = 'cs_films_found'
                    else: st.error("Failed to fetch showings.")
                st.rerun()

        if st.session_state.get('stage') == 'cs_films_found':
            st.subheader("Step 2: Choose Film Scope")
            
            film_sets = [set(s['film_title'] for s in st.session_state.all_showings.get(t['name'], [])) for t in st.session_state.compsnipe_theaters]
            all_films = sorted(list(set.union(*film_sets))) if film_sets else []
            common_films = sorted(list(set.intersection(*film_sets))) if film_sets else []
            
            c1, c2, c3 = st.columns(3)
            if c1.button(f"Scrape All {len(all_films)} Films", use_container_width=True, disabled=IS_DISABLED):
                st.session_state.selected_films = all_films
                st.session_state.compsnipe_film_filter_mode = 'all'
                st.session_state.stage = 'cs_showtimes'
                st.rerun()
            if c2.button(f"Scrape {len(common_films)} Common Films", use_container_width=True, disabled=IS_DISABLED):
                st.session_state.selected_films = common_films
                st.session_state.compsnipe_film_filter_mode = 'common'
                st.session_state.stage = 'cs_showtimes'
                st.rerun()
            if c3.button("Let Me Select Films...", use_container_width=True, disabled=IS_DISABLED):
                st.session_state.compsnipe_film_filter_mode = 'manual'
                st.session_state.stage = 'cs_showtimes'
                st.rerun()

        if st.session_state.get('stage') == 'cs_showtimes':
            st.subheader("Step 3: Select Films & Showtimes")

            if st.session_state.compsnipe_film_filter_mode == 'manual':
                film_sets = [set(s['film_title'] for s in st.session_state.all_showings.get(t['name'], [])) for t in st.session_state.compsnipe_theaters]
                all_films = sorted(list(set.union(*film_sets))) if film_sets else []
                st.write("Select Films:")
                cols = st.columns(4)
                for i, film in enumerate(all_films):
                    is_selected = film in st.session_state.selected_films
                    if cols[i % 4].button(film, key=f"cs_film_{film}", type="primary" if is_selected else "secondary", use_container_width=True, disabled=IS_DISABLED):
                        if is_selected: st.session_state.selected_films.remove(film)
                        else: st.session_state.selected_films.append(film)
                        st.rerun()
                st.divider()

            if st.session_state.selected_films:
                st.write("Auto-select showtimes by Daypart:")
                daypart_cols = st.columns(5)
                dayparts = ["All", "Matinee", "Twilight", "Prime", "Late Night"]
                for i, dp in enumerate(dayparts):
                    is_selected = dp in st.session_state.daypart_selections
                    if daypart_cols[i].button(dp, key=f"cs_dp_{dp}", type="primary" if is_selected else "secondary", use_container_width=True, disabled=IS_DISABLED):
                        handle_daypart_click(dp, st.session_state.all_showings, st.session_state.selected_films, [t['name'] for t in st.session_state.compsnipe_theaters])
                        st.rerun()

            for theater in st.session_state.get('compsnipe_theaters', []):
                theater_name = theater['name']
                has_selections = any(st.session_state.selected_showtimes.get(theater_name, {}).values())
                expander_label = f"‚úÖ  {theater_name}" if has_selections else f"‚ö™Ô∏è {theater_name}"
                with st.expander(expander_label, expanded=True):
                    showings = st.session_state.all_showings.get(theater_name, [])
                    films_to_display = {f for f in st.session_state.selected_films if f in [s['film_title'] for s in showings]}
                    if not films_to_display: st.write("No selected films are showing at this theater.")
                    for film in sorted(list(films_to_display)):
                        st.markdown(f"**{film}**")
                        film_showings = sorted([s for s in showings if s['film_title'] == film], key=lambda x: datetime.datetime.strptime(x['showtime'].replace('p', 'PM').replace('a', 'AM'), "%I:%M%p").time())
                        cols = st.columns(8)
                        for i, showing in enumerate(film_showings):
                            time_str = showing['showtime']
                            is_selected = time_str in st.session_state.selected_showtimes.get(theater_name, {}).get(film, {})
                            if cols[i % 8].button(time_str, key=f"cs_time_{theater_name}_{film}_{i}", type="primary" if is_selected else "secondary", use_container_width=True, disabled=IS_DISABLED):
                                if theater_name not in st.session_state.selected_showtimes: st.session_state.selected_showtimes[theater_name] = {}
                                if film not in st.session_state.selected_showtimes[theater_name]: st.session_state.selected_showtimes[theater_name][film] = {}
                                if is_selected: del st.session_state.selected_showtimes[theater_name][film][time_str]
                                else: st.session_state.selected_showtimes[theater_name][film][time_str] = showing
                                st.rerun()

            if any(any(film.values()) for film in st.session_state.get('selected_showtimes', {}).values()):
                st.subheader("Step 4: Generate Report")
                if st.button('üìÑ Generate Sniper Report', use_container_width=True, disabled=IS_DISABLED):
                    st.session_state.confirm_scrape = True
                    st.rerun()

    # --- This block handles the actual report generation after the UI has been locked ---
    if st.session_state.get('confirm_scrape'):
        st.info(f"You are about to scrape prices. This may take several minutes.")
        if st.button("‚úÖ Yes, Proceed", use_container_width=True, type="primary"):
            st.session_state.report_running = True
            st.session_state.confirm_scrape = False # Prevent this block from running again
            st.rerun()

    if st.session_state.report_running and not st.session_state.get('run_diagnostic'):
        mode = st.session_state.search_mode
        with st.spinner(f"Executing {mode} scrape... This may take several minutes. UI is locked."):
            theaters_for_report = []
            if mode == "CompSnipe Mode":
                theaters_for_report = st.session_state.compsnipe_theaters
            elif mode == "Market Mode":
                theaters_for_report = [t for t in st.session_state.theaters if t['name'] in st.session_state.selected_theaters]

            status, (result, showings_scraped), log, duration = run_async_in_thread(scout.scrape_details, theaters_for_report, st.session_state.selected_showtimes)
            st.session_state.last_run_log += log

            if status == 'success' and result:
                st.session_state.last_run_duration = duration
                log_runtime(mode, len(theaters_for_report), len(showings_scraped), duration)
                df_current = pd.DataFrame(result)
                
                if mode == "Market Mode" and st.session_state.get('selected_report_to_compare', "None") != "None":
                    df_previous = pd.read_csv(st.session_state.selected_report_to_compare)
                    merge_keys = ['Theater Name', 'Film Title', 'Showtime', 'Ticket Type']
                    df_merged = pd.merge(df_current, df_previous, on=merge_keys, how='left', suffixes=('_current', '_previous'))
                    df_merged['Price_current_num'] = pd.to_numeric(df_merged['Price_current'].str.replace('$', ''), errors='coerce')
                    df_merged['Price_previous_num'] = pd.to_numeric(df_merged['Price_previous'].str.replace('$', ''), errors='coerce')
                    df_merged['Price_Change'] = df_merged['Price_current_num'] - df_merged['Price_previous_num']
                    df_merged['Price_Change'] = df_merged['Price_Change'].apply(lambda x: f"${x:.2f}" if pd.notna(x) and x > 0 else (f"-${-x:.2f}" if pd.notna(x) and x < 0 else ("$0.00" if pd.notna(x) else "New")))
                    final_cols = ['Theater Name', 'Film Title', 'Showtime', 'Daypart_current', 'Ticket Type', 'Price_current', 'Price_previous', 'Price_Change', 'Capacity_current']
                    df_merged.rename(columns={'Price_current': 'Price', 'Daypart_current': 'Daypart', 'Price_previous': 'Previous Price', 'Capacity_current': 'Capacity'}, inplace=True)
                    st.session_state.final_df = df_merged[[col for col in final_cols if col in df_merged.columns]]
                else:
                    st.session_state.final_df = df_current
                    st.session_state.final_df['Previous Price'] = 'N/A'; st.session_state.final_df['Price_Change'] = 'N/A'
                
                timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H%M%S")
                report_path = get_report_path(mode, st.session_state.get('selected_region'), st.session_state.get('selected_market'))
                file_prefix = "report" if mode == "Market Mode" else "compsnipe_report"
                save_path = os.path.join(report_path, f"{file_prefix}_{timestamp}.csv")
                df_current.to_csv(save_path, index=False)
                st.session_state.last_saved_report = save_path 
                st.session_state.stage = 'report_generated'
            else:
                st.error(f"Scraper failed to produce a report after {duration:.2f} seconds.")

            st.session_state.report_running = False
            st.rerun()

    # --- Final Report Display ---
    if st.session_state.get('stage') == 'report_generated' and 'final_df' in st.session_state and not st.session_state.final_df.empty:
        st.header("Live Pricing Report")
        
        duration_str = f"(Took {st.session_state.last_run_duration:.2f} seconds)" if 'last_run_duration' in st.session_state else ""
        st.success(f"**Report Complete!** {duration_str} Data has been successfully saved to the database.")
        
        df_to_display = st.session_state.final_df
        st.dataframe(df_to_display, use_container_width=True)
        
        st.subheader("Download Report")
        col1, col2, _ = st.columns([1, 1, 4])
        
        excel_data = to_excel(df_to_display)
        csv_data = to_csv(df_to_display)
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")
        
        col1.download_button(
            label="üìÑ Download as CSV",
            data=csv_data,
            file_name=f'PriceScout_Report_{timestamp}.csv',
            mime='text/csv',
            use_container_width=True
        )
        
        col2.download_button(
            label="üìä Download as Excel",
            data=excel_data,
            file_name=f'PriceScout_Report_{timestamp}.xlsx',
            mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            use_container_width=True
        )

        st.balloons()

    if st.session_state.dev_mode and "last_run_log" in st.session_state:
        with st.expander("Developer Mode: Scraper Log"):
            st.code(st.session_state.last_run_log, language='text')
