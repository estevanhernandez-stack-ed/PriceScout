# price_scout_app.py - The Streamlit User Interface & Scraping Engine (v21.9.2 - Consolidated Report Message)

import streamlit as st
import pandas as pd
import os
import datetime
import time
import asyncio
import traceback
import sys
import threading
import urllib.parse
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import re
import json
from functools import reduce
import io
from contextlib import redirect_stdout
import requests # Using requests for fast static page scraping
import random

# --- Dynamically Define File Paths ---
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_DIR = os.path.dirname(SCRIPT_DIR)
DEBUG_DIR = os.path.join(PROJECT_DIR, 'debug_snapshots') # Directory for HTML snapshots

# --- Check for Developer Mode ---
query_params = st.query_params
DEV_MODE_ENABLED = query_params.get("dev") == "true"

# --- Streamlit Page Configuration ---
st.set_page_config(
    page_title="PriceScout",
    page_icon=os.path.join(SCRIPT_DIR, 'PriceScoutLogo.png'),
    layout="wide"
)

# --- Custom CSS ---
st.markdown("""
<style>
    /* --- Base Button Style --- */
    button {
        border-radius: 5px !important;
        font-weight: bold !important;
        transition: background-color 0.2s, color 0.2s, border-color 0.2s;
    }
    /* --- Unselected Button --- */
    button.st-emotion-cache-1anq8dj.e7nj0r42 {
        border: 2px solid #A21E25 !important;
        color: #A21E25 !important;
        background-color: transparent !important;
    }
    /* --- Selected Button --- */
    button.st-emotion-cache-1krtkoa.e7nj0r41 {
        background-color: #8B0F05 !important;
        color: white !important;
        border: 1px solid #8B0F05 !important;
    }
    /* --- Hover for All Buttons (Corrected) --- */
    button.st-emotion-cache-1anq8dj.e7nj0r42:hover,
    button.st-emotion-cache-1krtkoa.e7nj0r41:hover {
        background-color: #A21E25 !important;
        color: white !important;
        border-color: #A21E25 !important;
    }
    /* --- Focus Style (Corrected per Option 2) --- */
    button:focus {
        background-color: #8B0F05 !important; /* Dark Red */
        color: white !important;
        border-color: #8B0F05 !important;
        box-shadow: 0 0 0 0.2rem rgba(139, 15, 5, 0.5) !important;
    }

    /* --- Radio Buttons (Final) --- */
    /* Unselected circle */
    label.st-emotion-cache-1t3w24c .st-emotion-cache-7oyrr6 {
        border-color: #A21E25 !important;
    }
    /* Selected inner dot and outer circle */
    label.st-emotion-cache-1t3w24c input[type="radio"]:checked + div {
        background-color: #8B0F05 !important;
        border-color: #8B0F05 !important;
    }
    /* Text label of the selected option */
    label.st-emotion-cache-1t3w24c input[type="radio"]:checked ~ div.st-emotion-cache-1y4251l p {
        color: #8B0F05 !important;
        font-weight: bold !important;
    }

    /* --- Toggle Switch (Final) --- */
    /* Track of the toggle in the OFF state */
    label.st-emotion-cache-1t3w24c input[type="checkbox"] + div.st-emotion-cache-7oyrr6 {
        background-color: #f0f2f6 !important;
        border-color: #A21E25 !important;
    }
    /* Track of the toggle in the ON state */
    label.st-emotion-cache-1t3w24c input[type="checkbox"]:checked + div.st-emotion-cache-7oyrr6 {
        background-color: #A21E25 !important;
        border-color: #A21E25 !important;
    }
    /* Focus style for accessibility */
    label.st-emotion-cache-1t3w24c input[type="checkbox"]:focus + div.st-emotion-cache-7oyrr6 {
        box-shadow: 0 0 0 0.2rem rgba(139, 15, 5, 0.5) !important;
    }
</style>
""", unsafe_allow_html=True)


def check_password():
    """Returns `True` if the user has the correct password."""
    if "password" not in st.secrets:
        return True
    def password_entered():
        if st.session_state["password"] == st.secrets["password"]:
            st.session_state["password_correct"] = True
            del st.session_state["password"]
        else:
            st.session_state["password_correct"] = False
    if st.session_state.get("password_correct", False):
        return True
    st.text_input("Password", type="password", on_change=password_entered, key="password")
    if "password_correct" in st.session_state:
        st.error("üòï Password incorrect")
    return False

# --- Main Application Logic ---
if check_password():
    st.title('üìä PriceScout: Competitive Pricing Tool')

    # --- Constants ---
    CACHE_FILE = os.path.join(SCRIPT_DIR, 'theater_cache.json')
    MARKETS_FILE = os.path.join(PROJECT_DIR, 'data', 'Marcus', 'markets.json')
    CACHE_EXPIRATION_DAYS = 3
    REPORTS_DIR = os.path.join(PROJECT_DIR, 'data', 'Marcus', 'reports')

    # --- Data Scraping Engine (Scraper Class) ---
    class Scraper:
        def _sanitize_for_comparison(self, text: str) -> str:
            text = text.lower()
            text = re.sub(r'[^a-z0-9\s]', '', text)
            text = re.sub(r'\s\d+\s', ' ', text)
            text = re.sub(r'\s+', ' ', text).strip()
            return text

        def _sanitize_filename(self, name):
            return re.sub(r'[\\/*?:"<>|]',"", name).replace(" ", "_")

        def _parse_ticket_description(self, description: str) -> dict:
            desc_lower = description.lower()
            amenity_map = {
                'D-BOX': ['d-box', 'dbox'], 'IMAX': ['imax'], 'XD': ['xd'], 
                'Dolby Cinema': ['dolby'], 'Recliner': ['recliner'], 
                'Luxury': ['luxury'], '4DX': ['4dx'], 
                'Promotion': ['promotion', 'tuesday', 'unseen', 'fathom']
            }
            base_type_map = {'Adult': ['adult'], 'Child': ['child'], 'Senior': ['senior'], 'Military': ['military'], 'Student': ['student'], 'Matinee': ['matinee']}
            found_amenities = []
            remaining_desc = desc_lower
            for amenity, keywords in amenity_map.items():
                for keyword in keywords:
                    if keyword in remaining_desc:
                        found_amenities.append(amenity)
                        remaining_desc = remaining_desc.replace(keyword, '').strip()
            found_base_type = None
            for base_type, keywords in base_type_map.items():
                for keyword in keywords:
                    if keyword in remaining_desc:
                        found_base_type = base_type
                        remaining_desc = remaining_desc.replace(keyword, '').strip()
                        break
                if found_base_type: break
            if not found_base_type:
                remaining_desc = description.split('(')[0].strip()
                if remaining_desc.lower() in ['general admission', 'admission']:
                    found_base_type = "General Admission"
                else:
                    found_base_type = remaining_desc
            return {"base_type": found_base_type, "amenities": sorted(list(set(found_amenities)))}

        def _classify_daypart(self, showtime_str: str) -> str:
            try:
                s = showtime_str.strip().lower().replace('.', '')
                if s.endswith('p'): s = s[:-1] + 'pm'
                if s.endswith('a'): s = s[:-1] + 'am'
                s = s.replace('p.m.', 'pm').replace('a.m.', 'am').replace('p ', 'pm').replace('a ', 'am')
                if not any(x in s for x in ('am','pm')):
                    hour_match = re.match(r'(\d{1,2})', s)
                    if hour_match:
                        hour = int(hour_match.group(1))
                        s += 'am' if hour < 8 or hour == 12 else 'pm'
                t = datetime.datetime.strptime(s, "%I:%M%p").time()
                if t < datetime.time(16,0): return "Matinee"
                if t < datetime.time(18,0): return "Twilight"
                if t <= datetime.time(21,0): return "Prime"
                return "Late Night"
            except Exception as e:
                print(f"      [WARNING] Could not classify daypart for '{showtime_str}'. Error: {e}")
                return "Unknown"
        async def _get_theaters_from_zip_page(self, page, zip_code):
            url = f"https://www.fandango.com/{zip_code}_movietimes"
            print(f"  - Checking ZIP: {zip_code}")
            try:
                await page.goto(url, timeout=30000)
                await page.mouse.wheel(0, 2000)
                await page.wait_for_timeout(1500)
                js_condition = "() => window.Fandango && window.Fandango.pageDetails && window.Fandango.pageDetails.localTheaters && window.Fandango.pageDetails.localTheaters.length > 0"
                await page.wait_for_function(js_condition, timeout=20000)
                theaters_data = await page.evaluate('() => window.Fandango.pageDetails.localTheaters')
                return {t.get('name'): {"name": t.get('name'), "url": "https://www.fandango.com" + t.get('theaterPageUrl')} for t in theaters_data if t.get('name') and t.get('theaterPageUrl')}
            except Exception as e:
                print(f"    [WARNING] Could not process ZIP {zip_code}. Error: {e}")
                return {}
        async def live_search_by_zip(self, zip_code):
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                await page.goto(f"https://www.fandango.com/{zip_code}_movietimes", timeout=30000)
                await page.mouse.wheel(0, 2000)
                await page.wait_for_timeout(1500)
                js_condition = "() => window.Fandango && window.Fandango.pageDetails && window.Fandango.pageDetails.localTheaters && window.Fandango.pageDetails.localTheaters.length > 0"
                await page.wait_for_function(js_condition, timeout=20000)
                theaters_data = await page.evaluate('() => window.Fandango.pageDetails.localTheaters')
                results = {t.get('name'): {"name": t.get('name'), "url": "https://www.fandango.com" + t.get('theaterPageUrl')} for t in theaters_data if t.get('name') and t.get('theaterPageUrl')}
                await browser.close()
                return results

        async def live_search_by_name(self, search_term):
            print(f"  - Live searching for: {search_term}")
            results = {}
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                try:
                    await page.goto("https://www.fandango.com", timeout=60000)
                    await page.locator('[data-qa="search-input"]').fill(search_term)
                    await page.locator('[data-qa="search-input"]').press('Enter')
                    await page.wait_for_selector('[data-qa="search-results-item"]', timeout=15000)
                    soup = BeautifulSoup(await page.content(), 'html.parser')
                    search_results_items = soup.select('[data-qa="search-results-item"]')
                    for item in search_results_items:
                        link_elem = item.select_one('a[data-qa="search-results-item-link"]')
                        if link_elem and '/theater-page' in link_elem.get('href', ''):
                            name = link_elem.get_text(strip=True)
                            url = "https://www.fandango.com" + link_elem['href']
                            results[name] = {"name": name, "url": url}
                except Exception as e:
                    print(f"    [WARNING] Could not complete live name search. Error: {e}")
                await browser.close()
                return results

        def _scrape_state_index_page(self, state_abbr: str) -> dict:
            url = f"https://www.fandango.com/site-index/theaters/{state_abbr.lower()}.html"
            print(f"  - Scraping state index: {url}")
            theaters = {}
            try:
                response = requests.get(url, headers={'User-Agent': 'PriceScout Scraper'})
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                links = soup.select('a.site-index__link')
                for link in links:
                    name = link.get_text(strip=True)
                    href = link.get('href')
                    if name and href and '/theater-page' in href:
                        theaters[name] = {"name": name, "url": "https://www.fandango.com" + href}
            except requests.RequestException as e:
                print(f"    [ERROR] Could not fetch or parse state index for '{state_abbr}': {e}")
            return theaters

        async def build_theater_cache(self, markets_json_path):
            """
            Builds the theater cache using a hybrid "Fast-Then-Fallback" approach.
            1.  First, attempts to find theaters using an efficient ZIP code pool scrape.
            2.  For any theaters not found, it falls back to a targeted live name search.
            3.  A failsafe is run on the final combined results.
            """
            with open(markets_json_path, 'r') as f:
                markets_data = json.load(f)

            temp_cache = {"metadata": {"last_updated": datetime.datetime.now().isoformat()}, "markets": {}}
            total_theaters_to_find = 0
            total_theaters_found = 0

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()

                for parent_company, regions in markets_data.items():
                    for region_name, markets in regions.items():
                        for market_name, market_info in markets.items():
                            print(f"\n--- Processing Market: {market_name} ---")
                            theaters_in_market = market_info.get('theaters', [])
                            total_theaters_to_find += len(theaters_in_market)
                            
                            found_theaters_for_market = []
                            theaters_to_find_in_fallback = []

                            # --- PHASE 1: Fast ZIP Code Scrape ---
                            print("  [Phase 1] Starting fast ZIP code scrape...")
                            zip_pool = {t.get('zip') for t in theaters_in_market if t.get('zip')}
                            market_zip_cache = {}
                            for zip_code in zip_pool:
                                zip_results = await self._get_theaters_from_zip_page(page, zip_code)
                                market_zip_cache.update(zip_results)
                            
                            # Attempt to match theaters from the ZIP cache
                            for theater_from_json in theaters_in_market:
                                name_to_find = theater_from_json['name']
                                found = False
                                sanitized_target_name = self._sanitize_for_comparison(name_to_find)
                                for live_name, live_data in market_zip_cache.items():
                                    sanitized_live_name = self._sanitize_for_comparison(live_name)
                                    if sanitized_target_name in sanitized_live_name or sanitized_live_name in sanitized_target_name:
                                        found_theaters_for_market.append({'name': live_name, 'url': live_data['url']})
                                        found = True
                                        break
                                if not found:
                                    theaters_to_find_in_fallback.append(name_to_find)
                            
                            print(f"  [Phase 1] Found {len(found_theaters_for_market)} theaters via ZIP scrape.")

                            # --- PHASE 2: Targeted Fallback Name Search ---
                            if theaters_to_find_in_fallback:
                                print(f"  [Phase 2] Starting targeted fallback search for {len(theaters_to_find_in_fallback)} theater(s)...")
                                for theater_name in theaters_to_find_in_fallback:
                                    search_results = await self.live_search_by_name(theater_name)
                                    if search_results:
                                        found_name, found_data = next(iter(search_results.items()))
                                        print(f"    [SUCCESS] Fallback found '{theater_name}' as '{found_name}'")
                                        found_theaters_for_market.append({'name': found_name, 'url': found_data['url']})
                                    else:
                                        print(f"    [WARNING] Fallback could not find '{theater_name}'.")
                            
                            temp_cache["markets"][market_name] = {"theaters": found_theaters_for_market}
                            total_theaters_found += len(found_theaters_for_market)

                await browser.close()

            # --- FINAL STEP: The Failsafe Sanity Check ---
            print("\n--- Sanity Check ---")
            print(f"Found {total_theaters_found} out of {total_theaters_to_find} total theaters.")

            if total_theaters_to_find > 0 and (total_theaters_found / total_theaters_to_find) >= 0.75: # Higher threshold
                print("[SUCCESS] Sanity check passed. Overwriting old cache.")
                with open(CACHE_FILE, 'w') as f:
                    json.dump(temp_cache, f, indent=2)
                return temp_cache
            else:
                print("[FAILURE] Sanity check failed. Preserving existing cache to prevent errors.")
                return False

        async def _get_movies_from_theater_page(self, page, theater, date):
            full_url = f"{theater['url']}?date={date}"
            html_content = ""
            try:
                await page.goto(full_url, timeout=60000)
                await page.locator('div.theater-presenting-formats, li.fd-panel').first.wait_for(timeout=30000)
                html_content = await page.content()
                soup = BeautifulSoup(html_content, 'html.parser')
                showings = []
                movie_blocks = soup.select('li.fd-panel')
                if not movie_blocks and st.session_state.get('capture_html', False):
                    os.makedirs(DEBUG_DIR, exist_ok=True)
                    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"debug_{self._sanitize_filename(theater['name'])}_{timestamp}.html"
                    filepath = os.path.join(DEBUG_DIR, filename)
                    with open(filepath, 'w', encoding='utf-8') as f:
                        f.write(html_content)
                    print(f"  [DEBUG] No films found for {theater['name']}. Saved HTML snapshot to {filepath}")
                for movie_block in movie_blocks:
                    film_title_elem = movie_block.select_one('h2.thtr-mv-list__detail-title a')
                    film_title = film_title_elem.get_text(strip=True) if film_title_elem else "Unknown Title"
                    variant_title_elem = movie_block.select_one('.movie-variant-title')
                    variant_title = variant_title_elem.get_text(strip=True) if variant_title_elem else None
                    showtime_links = movie_block.select('ol.showtimes-btn-list a.showtime-btn')
                    for link in showtime_links:
                        time_label_elem = link.select_one('.showtime-btn-label')
                        amenity_elem = link.select_one('.showtime-btn-amenity')
                        time_str = time_label_elem.get_text(strip=True) if time_label_elem else link.get_text(strip=True)
                        amenity_str = amenity_elem.get_text(strip=True) if amenity_elem else variant_title
                        movie_format = amenity_str if amenity_str else "2D"
                        ticket_url = "https://tickets.fandango.com/transaction/ticketing/mobile/jump.aspx" + link.get('href', '').split('jump.aspx')[-1]
                        if film_title != "Unknown Title" and time_str and ticket_url and re.match(r'\d{1,2}:\d{2}[ap]m?', time_str, re.IGNORECASE):
                            showings.append({"film_title": film_title, "format": movie_format, "showtime": time_str, "daypart": self._classify_daypart(time_str), "ticket_url": ticket_url})
                return showings
            except Exception as e:
                print(f"    [ERROR] Failed to get movies for {theater['name']}. Error: {e}")
                return []

        async def _get_prices_and_capacity(self, page, showing_details):
            showtime_url = showing_details['ticket_url']
            results = {"tickets": [], "capacity": "N/A", "error": None}
            try:
                await page.goto(showtime_url, timeout=60000)
                await page.wait_for_timeout(random.randint(1500, 2500))
                html_content = await page.content()
                soup = BeautifulSoup(html_content, 'html.parser')
                scripts = soup.find_all('script')
                for script in scripts:
                    if script.string and 'window.Commerce.models' in script.string:
                        script_content = script.string
                        start_text = 'window.Commerce.models = '
                        start_index = script_content.find(start_text)
                        if start_index != -1:
                            json_start = script_content.find('{', start_index)
                            open_braces, json_end = 0, -1
                            for i in range(json_start, len(script_content)):
                                if script_content[i] == '{': open_braces += 1
                                elif script_content[i] == '}': open_braces -= 1
                                if open_braces == 0:
                                    json_end = i + 1; break
                            if json_end != -1:
                                data = json.loads(script_content[json_start:json_end])
                                ticket_types = data.get('tickets', {}).get('seatingAreas', [{}])[0].get('ticketTypes', [])
                                for tt in ticket_types:
                                    description, price = tt.get('description'), tt.get('price')
                                    if description and price is not None:
                                        parsed_ticket = self._parse_ticket_description(description)
                                        results["tickets"].append({
                                            "type": parsed_ticket["base_type"],
                                            "price": f"${price:.2f}",
                                            "amenities": parsed_ticket["amenities"]
                                        })
                                seating_info = data.get('seating', {})
                                total_seats = seating_info.get('totalSeats')
                                available_seats = seating_info.get('availableSeats')
                                if available_seats is not None and total_seats is not None:
                                    results["capacity"] = f"{available_seats} / {total_seats}"
                                if results["tickets"]: return results
            except Exception as e:
                results["error"] = f'Scraping failed: {e}'
            return results

        async def get_all_showings_for_theaters(self, theaters, date, mode):
            showings_by_theater = {}
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                for theater in theaters:
                    context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
                    page = await context.new_page()
                    showings = await self._get_movies_from_theater_page(page, theater, date)
                    if mode == "CompSnipe Mode":
                        for showing in showings:
                            showing['theater_name'] = theater['name']
                    showings_by_theater[theater['name']] = showings
                    await context.close()
            return showings_by_theater

        async def scrape_details(self, theaters, selected_showtimes, date):
            all_price_data = []
            showings_to_scrape = []
            for theater in theaters:
                if theater['name'] in selected_showtimes:
                    for film, times in selected_showtimes[theater['name']].items():
                        for time_str, showing_info in times.items():
                            showings_to_scrape.append({**showing_info, "theater_name": theater['name']})
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                for showing in showings_to_scrape:
                    context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
                    page = await context.new_page()
                    scrape_results = await self._get_prices_and_capacity(page, showing)
                    await context.close()
                    
                    if scrape_results["error"]:
                        print(f"  [ERROR] Scraping {showing['film_title']} at {showing['theater_name']}: {scrape_results['error']}")
                        continue
                    for ticket in scrape_results['tickets']:
                        initial_format = showing['format']
                        final_amenities = ticket['amenities']
                        combined_format_list = [initial_format] + final_amenities
                        unique_formats = sorted(list(set(combined_format_list)))
                        if len(unique_formats) > 1 and "2D" in unique_formats:
                            unique_formats.remove("2D")
                        all_price_data.append({
                            "Theater Name": showing['theater_name'], "Film Title": showing['film_title'],
                            "Format": ", ".join(unique_formats), "Showtime": showing['showtime'],
                            "Daypart": showing['daypart'], "Ticket Type": ticket['type'], "Price": ticket['price'],
                            "Capacity": scrape_results['capacity']
                        })
            return all_price_data

        async def run_diagnostic_scrape(self, markets_to_test, date):
            diagnostic_results = []
            theaters_to_test = []
            with open(CACHE_FILE, 'r') as f:
                cache = json.load(f)
            for market_name in markets_to_test:
                theaters = cache.get("markets", {}).get(market_name, {}).get("theaters", [])
                for theater in theaters:
                    theater['market'] = market_name
                    theaters_to_test.append(theater)
           
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                for i, theater in enumerate(theaters_to_test):
                    print(f"Testing {i+1}/{len(theaters_to_test)}: {theater['name']}")
                    context = await browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
                    page = await context.new_page()
                    result_row = {"Market": theater['market'], "Theater Name": theater['name'], "Status": "Failed", "Details": "No showtimes found", "Sample Price": "N/A"}
                    try:
                        showings = await self._get_movies_from_theater_page(page, theater, date)
                        if showings:
                            first_showing = showings[0]
                            price_results = await self._get_prices_and_capacity(page, first_showing)
                            if price_results['tickets']:
                                first_ticket = price_results['tickets'][0]
                                result_row.update({
                                    "Status": "Success",
                                    "Details": f"Scraped '{first_showing['film_title']}' at {first_showing['showtime']}",
                                    "Sample Price": f"{first_ticket['type']}: {first_ticket['price']}"
                                })
                            else:
                                result_row["Details"] = "Failed to extract price from ticket page."
                        diagnostic_results.append(result_row)
                    except Exception as e:
                        result_row["Details"] = f"An unexpected error occurred: {str(e)}"
                        diagnostic_results.append(result_row)
                    finally:
                        await context.close()
            return diagnostic_results

    scout = Scraper()

    # --- UI Helper Functions ---
    def run_async_in_thread(target_func, *args):
        """
        Runs an async function in a separate thread and captures its output,
        status, log, and execution duration.
        """
        log_stream = io.StringIO()
        status, value, duration = 'error', None, 0.0

        def thread_target():
            nonlocal status, value, duration
            start_time = time.time()
            if sys.platform == "win32":
                asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
            try:
                with redirect_stdout(log_stream):
                    # For single coroutines, we need to handle them differently
                    if len(args) == 1 and asyncio.iscoroutine(args[0]):
                         result = asyncio.run(args[0])
                    else:
                         result = asyncio.run(target_func(*args))
                duration = time.time() - start_time
                status, value = 'success', result
            except Exception:
                duration = time.time() - start_time
                error_str = traceback.format_exc()
                print(f"\n--- TRACEBACK ---\n{error_str}", file=log_stream)
                status, value = 'error', error_str

        thread = threading.Thread(target=thread_target)
        thread.start()
        thread.join()
        log_output = log_stream.getvalue()
        return status, value, log_output, duration

    def check_cache_status():
        if not os.path.exists(CACHE_FILE):
            return "missing", None
        try:
            with open(CACHE_FILE, 'r') as f:
                cache_data = json.load(f)
            last_updated_str = cache_data.get("metadata", {}).get("last_updated")
            if not last_updated_str:
                return "invalid", None
            last_updated = datetime.datetime.fromisoformat(last_updated_str)
            if (datetime.datetime.now() - last_updated).days >= CACHE_EXPIRATION_DAYS:
                return "stale", last_updated.strftime('%Y-%m-%d %H:%M')
            return "fresh", last_updated.strftime('%Y-%m-%d %H:%M')
        except (json.JSONDecodeError, KeyError):
            return "invalid", None

    def get_report_path(mode, region=None, market=None):
        if mode == "Market Mode":
            path = os.path.join(REPORTS_DIR, "MarketMode", scout._sanitize_filename(region or ""), scout._sanitize_filename(market or ""))
        elif mode == "CompSnipe Mode":
            path = os.path.join(REPORTS_DIR, "SnipeMode")
        else:
            path = os.path.join(REPORTS_DIR, "misc")
        os.makedirs(path, exist_ok=True)
        return path

    def reset_session():
        # --- CHANGE START ---
        # Added 'last_run_duration' to the reset list for good housekeeping.
        st.session_state.search_mode = "Market Mode"
        keys_to_reset = [
            'stage', 'selected_region', 'selected_market', 'theaters',
            'selected_theaters', 'all_showings', 'selected_films',
            'selected_showtimes', 'confirm_scrape', 'compsnipe_theaters',
            'compsnipe_name_search_term', 'live_search_results', 'last_mode',
            'show_failsafe', 'run_diagnostic', 'last_run_duration'
        ]
        # --- CHANGE END ---
        for key in keys_to_reset:
            if key in st.session_state:
                del st.session_state[key]
        if 'final_df' in st.session_state:
            del st.session_state.final_df
        st.rerun()

    def get_market_mode_warnings(selected_theaters, all_showings, selected_films):
        if not selected_films:
            return []
        theaters_awaiting = []
        for theater_name in selected_theaters:
            has_film = any(film in [s['film_title'] for s in all_showings.get(theater_name, [])] for film in selected_films)
            if not has_film:
                theaters_awaiting.append(theater_name)
        return theaters_awaiting

    def style_price_change(val):
        if isinstance(val, str):
            if val.startswith('-$'):
                return 'color: green; font-weight: bold;'
            elif val.startswith('$') and val != '$0.00':
                return 'color: red; font-weight: bold;'
        return ''

    async def _get_theater_address(theater_url):
        address_info = {"state": "Unknown", "zip": "Unknown"}
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            try:
                await page.goto(theater_url, timeout=20000)
                address_selector = 'div[data-qa="theater-details-address"]'
                await page.wait_for_selector(address_selector, timeout=15000)
                address_text = await page.inner_text(address_selector)
                state_zip_match = re.search(r',\s*([A-Z]{2})\s*(\d{5})', address_text)
                if state_zip_match:
                    address_info["state"] = state_zip_match.group(1)
                    address_info["zip"] = state_zip_match.group(2)
            except Exception as e:
                print(f"    [INFO] Could not scrape address for {theater_url}. Error: {e}")
            await browser.close()
        return address_info

    def update_cache_with_snipe_results(theaters_to_add):
        if not theaters_to_add:
            return 0
        try:
            with open(CACHE_FILE, 'r') as f:
                cache_data = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            cache_data = {"metadata": {"last_updated": ""}, "markets": {}}
        
        all_existing_urls = set()
        for market_data in cache_data.get("markets", {}).values():
            if "theaters" in market_data:
                for theater in market_data.get("theaters", []):
                    all_existing_urls.add(theater.get("url"))
            else:
                for state_data in market_data.values():
                    for zip_data in state_data.values():
                        for theater in zip_data.get("theaters", []):
                            all_existing_urls.add(theater.get("url"))

        added_count = 0
        theaters_to_categorize = [t for t in theaters_to_add if t.get("url") not in all_existing_urls]
        
        if not theaters_to_categorize:
            st.toast("All selected theaters are already in the cache.")
            return 0
        
        coroutines = [_get_theater_address(t['url']) for t in theaters_to_categorize]
        status, addresses, log, _ = run_async_in_thread(asyncio.gather, *coroutines)

        st.session_state.last_run_log += log
        if status == 'error' or not addresses:
            st.error("Failed to fetch addresses for new theaters.")
            return 0
        if "Out-of-Market Theaters" not in cache_data["markets"]:
            cache_data["markets"]["Out-of-Market Theaters"] = {}
        for i, theater in enumerate(theaters_to_categorize):
            address = addresses[i]
            state, zip_code = address['state'], address['zip']
            if state not in cache_data["markets"]["Out-of-Market Theaters"]:
                cache_data["markets"]["Out-of-Market Theaters"][state] = {}
            if zip_code not in cache_data["markets"]["Out-of-Market Theaters"][state]:
                cache_data["markets"]["Out-of-Market Theaters"][state][zip_code] = {"theaters": []}
            cache_data["markets"]["Out-of-Market Theaters"][state][zip_code]["theaters"].append(theater)
            added_count += 1
        if added_count > 0:
            print(f"  [CACHE UPDATE] Added {added_count} new theater(s) to 'Out-of-Market Theaters'.")
            cache_data["metadata"]["last_updated"] = datetime.datetime.now().isoformat()
            with open(CACHE_FILE, 'w') as f:
                json.dump(cache_data, f, indent=2)
        return added_count

    # --- Session State Initialization ---
    if 'stage' not in st.session_state: st.session_state.stage = 'initial'
    if 'search_mode' not in st.session_state: st.session_state.search_mode = "Market Mode"
    if 'last_run_log' not in st.session_state: st.session_state.last_run_log = ""
    if 'dev_mode' not in st.session_state: st.session_state.dev_mode = False
    if 'capture_html' not in st.session_state: st.session_state.capture_html = False
    if 'confirm_scrape' not in st.session_state: st.session_state.confirm_scrape = False
    if 'compsnipe_theaters' not in st.session_state: st.session_state.compsnipe_theaters = []
    if 'live_search_results' not in st.session_state: st.session_state.live_search_results = {}
    if 'compsnipe_selected_theaters' not in st.session_state: st.session_state.compsnipe_selected_theaters = []
    if 'compsnipe_selected_dayparts' not in st.session_state: st.session_state.compsnipe_selected_dayparts = ["All"]


    # --- Load initial data ---
    try:
        with open(MARKETS_FILE, 'r') as f:
            markets_data = json.load(f)
        with open(CACHE_FILE, 'r') as f:
            cache_data = json.load(f)
    except FileNotFoundError:
        st.error(f"Error: `{MARKETS_FILE}` or `{CACHE_FILE}` not found. Please ensure they are in the same directory.")
        st.info("You may need to build the theater cache first if it's missing.")
        st.stop()


    # --- Main UI ---
    st.sidebar.title("Controls")
    st.sidebar.image(os.path.join(SCRIPT_DIR, 'PriceScoutLogo.png'))
    if st.sidebar.button("üöÄ Start New Report", use_container_width=True):
        reset_session()

    st.sidebar.divider()

    if DEV_MODE_ENABLED:
        st.sidebar.header("Developer Tools")
        st.session_state.dev_mode = True
        st.session_state.capture_html = st.sidebar.toggle("Capture HTML Snapshots", help="Save HTML files for analysis.")
        if st.sidebar.button("Run Full System Diagnostic", use_container_width=True):
            st.session_state.run_diagnostic = True
            st.rerun()
    else:
        st.session_state.dev_mode = False

    if st.session_state.get('run_diagnostic'):
        st.header("üõ†Ô∏è Full System Diagnostic")
        st.warning("**Warning:** This will scrape every theater in the selected markets and may take a very long time to complete.")
        all_markets = list(markets_data["Marcus Theatres"]["South MT"].keys()) + list(markets_data["Marcus Theatres"]["North MT"].keys())
        markets_to_test = st.multiselect("Select markets to test:", options=all_markets, default=all_markets)
        if st.button("Start Full Diagnostic", type="primary"):
            with st.spinner("Running diagnostic scan... This will take a long time."):
                diag_date_str = (datetime.date.today() + datetime.timedelta(days=1)).strftime('%Y-%m-%d')
                status, result, log, duration = run_async_in_thread(scout.run_diagnostic_scrape, markets_to_test, diag_date_str)
                st.session_state.last_run_log += log
                if status == 'success':
                    st.success(f"Diagnostic Complete! (Took {duration:.2f} seconds)")
                    df_diag = pd.DataFrame(result)
                    st.dataframe(df_diag)
                else:
                    st.error(f"The diagnostic tool encountered a critical error after {duration:.2f} seconds.")
        st.stop()


    st.subheader("Theater Data Cache")
    cache_status, last_updated = check_cache_status()
    if cache_status == "fresh":
        st.success(f"Theater cache is up to date. Last refreshed: {last_updated}")
    elif cache_status == "stale":
        st.warning(f"Theater cache is stale (older than {CACHE_EXPIRATION_DAYS} days). Last refreshed: {last_updated}")
    else:
        st.error("Theater cache file is missing or invalid. Please build it.")

    if st.button("Build / Refresh Theater Cache"):
        st.session_state.confirm_scrape = False
        with st.spinner("Building theater cache... grab a coffee!"):
            status, result, log, duration = run_async_in_thread(scout.build_theater_cache, MARKETS_FILE)
            st.session_state.last_run_log = log
            if status == 'success':
                st.success(f"Theater cache built successfully in {duration:.2f} seconds!")
                st.rerun()
            else:
                st.error(f"Failed to build theater cache after {duration:.2f} seconds.")
    st.divider()

    st.subheader("Step 1: Define Search Area")
    cols_mode = st.columns(2)
    is_market_mode = st.session_state.search_mode == "Market Mode"
    if cols_mode[0].button("Market Mode", use_container_width=True, type="primary" if is_market_mode else "secondary"):
        if not is_market_mode:
            st.session_state.search_mode = "Market Mode"
            st.session_state.confirm_scrape = False # Reset confirmation on mode switch
            st.rerun()

    is_compsnipe_mode = st.session_state.search_mode == "CompSnipe Mode"
    if cols_mode[1].button("CompSnipe Mode", use_container_width=True, type="primary" if is_compsnipe_mode else "secondary"):
        if not is_compsnipe_mode:
            st.session_state.search_mode = "CompSnipe Mode"
            st.session_state.confirm_scrape = False # Reset confirmation on mode switch
            st.rerun()

    # ==============================================================================
    # --- MARKET MODE ---
    # ==============================================================================
    if st.session_state.search_mode == "Market Mode":
        if 'selected_region' not in st.session_state: st.session_state.selected_region = None
        if 'selected_market' not in st.session_state: st.session_state.selected_market = None
        if 'theaters' not in st.session_state: st.session_state.theaters = []
        if 'selected_theaters' not in st.session_state: st.session_state.selected_theaters = []
        if 'all_showings' not in st.session_state: st.session_state.all_showings = {}
        if 'selected_films' not in st.session_state: st.session_state.selected_films = []
        if 'selected_showtimes' not in st.session_state: st.session_state.selected_showtimes = {}
        
        parent_company = list(markets_data.keys())[0]
        regions = list(markets_data[parent_company].keys())
        cols = st.columns(len(regions))
        for i, region in enumerate(regions):
            is_selected = st.session_state.selected_region == region
            if cols[i].button(region, key=f"region_{region}", type="primary" if is_selected else "secondary", use_container_width=True):
                st.session_state.selected_region = region
                st.session_state.selected_market = None
                st.session_state.stage = 'region_selected'
                st.rerun()

        if st.session_state.selected_region:
            st.markdown(f"### Region: {st.session_state.selected_region}")
            markets = list(markets_data[parent_company][st.session_state.selected_region].keys())
            st.write("Select a Market:")
            market_cols = st.columns(4)
            for i, market in enumerate(markets):
                is_selected = st.session_state.selected_market == market
                if market_cols[i % 4].button(market, key=f"market_{market}", type="primary" if is_selected else "secondary", use_container_width=True):
                    st.session_state.selected_market = market
                    st.session_state.theaters = cache_data.get("markets", {}).get(market, {}).get("theaters", [])
                    st.session_state.selected_theaters = [t['name'] for t in st.session_state.theaters]
                    st.session_state.stage = 'theaters_listed'
                    st.rerun()

        if st.session_state.stage in ['theaters_listed', 'data_fetched', 'report_generated']:
            st.subheader("Step 2: Select Theaters")
            cols = st.columns(4)
            for i, theater in enumerate(st.session_state.theaters):
                is_selected = theater['name'] in st.session_state.selected_theaters
                if cols[i % 4].button(theater['name'], key=f"theater_{i}", type="primary" if is_selected else "secondary", use_container_width=True):
                    if is_selected: st.session_state.selected_theaters.remove(theater['name'])
                    else: st.session_state.selected_theaters.append(theater['name'])
                    st.rerun()
            
            matching_films_only_market = st.toggle("Only Scrape Films Playing at ALL Selected Theaters", key="market_matching_films")

            scrape_date = st.date_input("Select Date for Showtimes", datetime.date.today() + datetime.timedelta(days=1))
            scrape_date_str = scrape_date.strftime('%Y-%m-%d')

            if st.button("Find Films for Selected Theaters"):
                theaters_to_scrape = [t for t in st.session_state.theaters if t['name'] in st.session_state.selected_theaters]
                with st.spinner("Finding all available films and showtimes..."):
                    status, result, log, duration = run_async_in_thread(scout.get_all_showings_for_theaters, theaters_to_scrape, scrape_date_str, "Market Mode")
                    st.session_state.last_run_log = log
                    if status == 'success':
                        st.info(f"Film search completed in {duration:.2f} seconds.")
                        st.session_state.all_showings = result
                        st.session_state.selected_films = []
                        st.session_state.selected_showtimes = {}
                        st.session_state.stage = 'data_fetched'
                    else: st.error("Failed to fetch showings for theaters.")
                st.rerun()

        if st.session_state.stage in ['data_fetched', 'report_generated']:
            st.subheader("Step 3: Select Films & Showtimes")
            
            theaters_awaiting = get_market_mode_warnings(st.session_state.selected_theaters, st.session_state.all_showings, st.session_state.selected_films)
            if theaters_awaiting:
                st.warning("Theaters Awaiting a Film Selection:")
                st.markdown("\n".join(f"- **{theater}**" for theater in theaters_awaiting))

            all_films = sorted(list(reduce(lambda a, b: a.union(b), [set(s['film_title'] for s in showings) for showings in st.session_state.all_showings.values() if showings], set())))
            st.write("Select Films:")
            cols = st.columns(4)
            for i, film in enumerate(all_films):
                is_selected = film in st.session_state.selected_films
                if cols[i % 4].button(film, key=f"film_{film}", type="primary" if is_selected else "secondary", use_container_width=True):
                    if is_selected: st.session_state.selected_films.remove(film)
                    else: st.session_state.selected_films.append(film)
                    st.rerun()
            st.divider()

            def handle_showtime_click(theater_name, film_title, time_str, showing_info):
                if theater_name not in st.session_state.selected_showtimes: st.session_state.selected_showtimes[theater_name] = {}
                if film_title not in st.session_state.selected_showtimes[theater_name]: st.session_state.selected_showtimes[theater_name][film_title] = {}
                if time_str in st.session_state.selected_showtimes[theater_name][film_title]: del st.session_state.selected_showtimes[theater_name][film_title][time_str]
                else: st.session_state.selected_showtimes[theater_name][film_title][time_str] = showing_info

            for theater_name in st.session_state.selected_theaters:
                has_selections = any(st.session_state.selected_showtimes.get(theater_name, {}).values())
                expander_label = f"‚úÖ  {theater_name}" if has_selections else f"‚ö™Ô∏è {theater_name}"
                with st.expander(expander_label, expanded=True):
                    showings = st.session_state.all_showings.get(theater_name, [])
                    films_to_display = {f for f in st.session_state.selected_films if f in [s['film_title'] for s in showings]}
                    if not films_to_display: st.write("No selected films are showing at this theater.")
                    for film in sorted(list(films_to_display)):
                        st.markdown(f"**{film}**")
                        film_showings = sorted([s for s in showings if s['film_title'] == film], key=lambda x: datetime.datetime.strptime(x['showtime'].replace('p', 'PM').replace('a', 'AM'), "%I:%M%p").time())
                        cols = st.columns(8)
                        for i, showing in enumerate(film_showings):
                            time_str = showing['showtime']
                            is_selected = time_str in st.session_state.selected_showtimes.get(theater_name, {}).get(film, {})
                            if cols[i % 8].button(time_str, key=f"time_{theater_name}_{film}_{i}", type="primary" if is_selected else "secondary", use_container_width=True):
                                handle_showtime_click(theater_name, film, time_str, showing)
                                st.session_state.confirm_scrape = False
                                st.rerun()

        if any(any(film.values()) for film in st.session_state.get('selected_showtimes', {}).values()):
            st.subheader("Step 4: Generate Report")
            report_path = get_report_path(st.session_state.search_mode, st.session_state.get('selected_region'), st.session_state.get('selected_market'))
            previous_reports = []
            if report_path and os.path.exists(report_path):
                previous_reports = sorted([os.path.join(report_path, f) for f in os.listdir(report_path) if f.endswith('.csv')], reverse=True)
            selected_report_to_compare = st.selectbox("Compare to Previous Report (Optional):", ["None"] + previous_reports, help="Select a previous report to compare prices against.")
            if st.button('üìÑ Generate Live Pricing Report', use_container_width=True):
                st.session_state.confirm_scrape = True
                st.rerun()

# ==============================================================================
    # --- COMPSNIPE MODE ---
    # ==============================================================================
    elif st.session_state.search_mode == "CompSnipe Mode":
        st.info("Use this mode for targeted analysis of individual theaters by searching for competitors within a ZIP code.")
        
        zip_col, zip_btn_col = st.columns([4, 1])
        with zip_col:
            zip_search_term = st.text_input("Enter 5-digit ZIP code to find theaters", max_chars=5, key="zip_search_input",
                                            on_change=lambda: st.session_state.update(live_search_results={}, compsnipe_selected_theaters=[]))
        with zip_btn_col:
            st.write("") 
            if st.button("Search by ZIP", key="search_by_zip_btn"):
                st.session_state.live_search_results = {}
                st.session_state.compsnipe_selected_theaters = [] 
                if zip_search_term:
                    with st.spinner(f"Live searching Fandango for theaters near {zip_search_term}..."):
                        status, result, log, _ = run_async_in_thread(scout.live_search_by_zip, zip_search_term)
                        st.session_state.last_run_log = log
                        if status == 'success':
                            st.session_state.live_search_results = result
                            if not result:
                                st.warning(f"No theaters found for ZIP code '{zip_search_term}'.")
                        else:
                            st.error("Failed to perform live ZIP search.")
                else:
                    st.warning("Please enter a ZIP code.")

        if st.session_state.live_search_results:
            st.subheader("Step 2: Select Theaters to Snipe")

            if 'compsnipe_selected_theaters' not in st.session_state:
                st.session_state.compsnipe_selected_theaters = []

            cols = st.columns(4)
            for i, name in enumerate(sorted(st.session_state.live_search_results.keys())):
                is_selected = name in st.session_state.compsnipe_selected_theaters
                if cols[i % 4].button(name, key=f"cs_theater_{i}", type="primary" if is_selected else "secondary", use_container_width=True):
                    if is_selected:
                        st.session_state.compsnipe_selected_theaters.remove(name)
                    else:
                        st.session_state.compsnipe_selected_theaters.append(name)
                    st.rerun()
            
            st.session_state.compsnipe_theaters = [st.session_state.live_search_results[name] for name in st.session_state.compsnipe_selected_theaters]

        if st.session_state.compsnipe_theaters:
            st.subheader("Step 3: Define Snipe Parameters")
            scrape_date_cs = st.date_input("Select Date for Showtimes", datetime.date.today() + datetime.timedelta(days=1), key="cs_date")
            scrape_date_cs_str = scrape_date_cs.strftime('%Y-%m-%d')
            
            matching_films_only = st.toggle("Only Scrape Films Playing at ALL Selected Theaters", key="cs_matching_films", help="When enabled, the report will only include films that are available at every single theater you have selected.")

            if st.button("Sniper Report: Get All Showings & Prices", use_container_width=True):
                st.session_state.confirm_scrape = True
                st.rerun()
                
            if st.session_state.get('confirm_scrape'):
                st.info(f"You are about to scrape ALL showings and prices for **{len(st.session_state.compsnipe_theaters)}** selected theater(s) on **{scrape_date_cs_str}**.")
                if st.button("‚úÖ Yes, Proceed with Snipe", use_container_width=True, type="primary"):
                    with st.spinner("Executing sniper scrape... This may take several minutes."):
                        theaters = st.session_state.compsnipe_theaters
                        status_s, showings_data, log_s, duration_s = run_async_in_thread(scout.get_all_showings_for_theaters, theaters, scrape_date_cs_str, "CompSnipe Mode")
                        st.session_state.last_run_log += log_s
                        
                        if status_s == 'success':
                            films_to_scrape = set()
                            if matching_films_only:
                                film_sets = [set(s['film_title'] for s in showings) for showings in showings_data.values()]
                                if film_sets:
                                    films_to_scrape = set.intersection(*film_sets)
                                    st.info(f"Found {len(films_to_scrape)} films playing at all selected theaters.")
                            else:
                                films_to_scrape = {s['film_title'] for showings in showings_data.values() for s in showings}

                            showtimes_for_scrape = {t_name: {} for t_name in showings_data}
                            for t_name, film_list in showings_data.items():
                                for film_showing in film_list:
                                    if film_showing['film_title'] in films_to_scrape:
                                        film_title = film_showing['film_title']
                                        if film_title not in showtimes_for_scrape[t_name]: 
                                            showtimes_for_scrape[t_name][film_title] = {}
                                        showtimes_for_scrape[t_name][film_title][film_showing['showtime']] = film_showing
                            
                            status_p, result, log_p, duration_p = run_async_in_thread(scout.scrape_details, theaters, showtimes_for_scrape, scrape_date_cs_str)
                            st.session_state.last_run_log += log_p
                            total_duration = duration_s + duration_p
                            if status_p == 'success' and result:
                                # --- CHANGE START ---
                                # Storing duration in session state instead of displaying it here.
                                st.session_state.last_run_duration = total_duration
                                # --- CHANGE END ---
                                df = pd.DataFrame(result)
                                df['Previous Price'] = 'N/A'
                                df['Price_Change'] = 'N/A'
                                st.session_state.final_df = df
                                report_path = get_report_path(st.session_state.search_mode)
                                timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H%M%S")
                                save_path = os.path.join(report_path, f"compsnipe_report_{timestamp}.csv")
                                st.session_state.final_df.to_csv(save_path, index=False)
                                st.session_state.last_saved_report = save_path
                                st.session_state.stage = 'report_generated'
                            else: st.error(f"Sniper scrape failed to produce a report after {total_duration:.2f} seconds.")
                        else: st.error(f"Failed to fetch showings for CompSnipe mode after {duration_s:.2f} seconds.")
                    st.session_state.confirm_scrape = False
                    st.rerun()

    # ==============================================================================
    # --- REPORT DISPLAY & GENERATION (Market Mode Continuation) ---
    # ==============================================================================
    if st.session_state.get('confirm_scrape') and st.session_state.search_mode == "Market Mode":
        total_showings = sum(len(times) for films in st.session_state.selected_showtimes.values() for times in films.values())
        total_theaters = len(st.session_state.selected_showtimes)
        st.info(f"You are about to scrape **{total_showings}** showtimes across **{total_theaters}** theaters. This may take a moment.")
        if st.button("‚úÖ Yes, Proceed with Scrape", use_container_width=True, type="primary"):
            with st.spinner("Running detailed scrape..."):
                theaters_for_report = [t for t in st.session_state.theaters if t['name'] in st.session_state.selected_theaters]
                status, result, log, duration = run_async_in_thread(scout.scrape_details, theaters_for_report, st.session_state.selected_showtimes, scrape_date_str)
                st.session_state.last_run_log = log
                st.session_state.confirm_scrape = False
                if status == 'success' and result:
                    # --- CHANGE START ---
                    # Storing duration in session state instead of displaying it here.
                    st.session_state.last_run_duration = duration
                    # --- CHANGE END ---
                    df_current = pd.DataFrame(result)
                    df_previous = pd.DataFrame() 
                    if 'selected_report_to_compare' in locals() and selected_report_to_compare != "None":
                        df_previous = pd.read_csv(selected_report_to_compare)
                        merge_keys = ['Theater Name', 'Film Title', 'Showtime', 'Ticket Type']
                        df_merged = pd.merge(df_current, df_previous, on=merge_keys, how='left', suffixes=('_current', '_previous'))
                        df_merged['Price_current_num'] = pd.to_numeric(df_merged['Price_current'].str.replace('$', ''), errors='coerce')
                        df_merged['Price_previous_num'] = pd.to_numeric(df_merged['Price_previous'].str.replace('$', ''), errors='coerce')
                        df_merged['Price_Change'] = df_merged['Price_current_num'] - df_merged['Price_previous_num']
                        df_merged['Price_Change'] = df_merged['Price_Change'].apply(lambda x: f"${x:.2f}" if pd.notna(x) and x > 0 else (f"-${-x:.2f}" if pd.notna(x) and x < 0 else ("$0.00" if pd.notna(x) else "New")))
                        final_cols = ['Theater Name', 'Film Title', 'Showtime', 'Daypart_current', 'Ticket Type', 'Price_current', 'Price_previous', 'Price_Change', 'Capacity_current']
                        df_merged.rename(columns={'Price_current': 'Price', 'Daypart_current': 'Daypart', 'Price_previous': 'Previous Price', 'Capacity_current': 'Capacity'}, inplace=True)
                        st.session_state.final_df = df_merged[[col for col in final_cols if col in df_merged.columns]]
                    else:
                        st.session_state.final_df = df_current
                        st.session_state.final_df['Previous Price'] = 'N/A'
                        st.session_state.final_df['Price_Change'] = 'N/A'
                    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H%M%S")
                    save_path = os.path.join(report_path, f"report_{timestamp}.csv")
                    df_current.to_csv(save_path, index=False)
                    st.session_state.last_saved_report = save_path 
                    st.session_state.stage = 'report_generated'
                else: st.error(f"Scraper failed to produce a report after {duration:.2f} seconds.")
            st.rerun()

    if st.session_state.get('stage') == 'report_generated' and 'final_df' in st.session_state and not st.session_state.final_df.empty:
        st.header("Live Pricing Report")
        if st.session_state.search_mode == "Market Mode":
            warnings = get_market_mode_warnings(st.session_state.selected_theaters, st.session_state.all_showings, st.session_state.selected_films)
            if warnings:
                st.warning("Theaters Awaiting a Film Selection:")
                st.markdown("\n".join(f"- **{theater}**" for theater in warnings))
        # --- CHANGE START ---
        # Modified the success message to include the duration.
        if 'last_saved_report' in st.session_state:
            duration_str = f"(Took {st.session_state.last_run_duration:.2f} seconds)" if 'last_run_duration' in st.session_state else ""
            st.success(f"**Report Complete!** {duration_str} Data has been successfully saved to the following location:")
            st.code(st.session_state.last_saved_report, language='text')
        # --- CHANGE END ---
        
        df_to_display = st.session_state.final_df
        if 'Price_Change' in df_to_display.columns:
            st.dataframe(df_to_display.style.map(style_price_change, subset=['Price_Change']), use_container_width=True)
        else:
            st.dataframe(df_to_display, use_container_width=True)

        st.balloons()

    if st.session_state.dev_mode and "last_run_log" in st.session_state:
        with st.expander("Developer Mode: Scraper Log"):
            st.code(st.session_state.last_run_log, language='text')